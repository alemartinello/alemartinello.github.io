[
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Over the years I have worked on many different fields, from household finance to refugee resettlement. Here you can find my published work, togeher with a long-ish list of pretty-much-done projects that were however never submitted–or submitted only once or twice before I abandoned academia. I refer to these as resting in the elephants’ graveyard."
  },
  {
    "objectID": "research.html#publications",
    "href": "research.html#publications",
    "title": "Research",
    "section": "Publications",
    "text": "Publications\n\n\n\n\n\n\nSubstantial transmission of SARS-CoV-2 through casual contact in retail stores: Evidence from matched administrative microdata on card payments and testingPNAS (2024) 121 (17)\n\n\n\n\n\nwith Niels Johannesen, Bjørn Bjørnsson Meyer, Emil Toft Vestergaard, Asger Lau Andersen, and Thais Lærkholm Jensen\nAbstract This paper presents quasiexperimental evidence of Covid-19 transmission through casual contact between customers in retail stores. For a large sample of individuals in Denmark, we match card payment data, indicating exactly where and when each individual made purchases, with Covid-19 test data, indicating when each individual was tested and whether the test was positive. The resulting dataset identifies more than 100,000 instances where an infected individual made a purchase in a store and, in each instance, allows us to track the infection dynamics of other individuals who made purchases in the same store around the same time. We estimate transmissions by comparing the infection rate of exposed customers, who made a purchase within 5 min of an infected individual, and nonexposed customers, who made a purchase in the same store 16 to 30 min before. We find that exposure to an infected individual in a store increases the infection rate by around 0.12 percentage points (P &lt; 0.001) between day 3 and day 7 after exposure. The estimates imply that transmissions in stores contributed around 0.04 to the reproduction number for the average infected individual and significantly more in the period where Omicron was the dominant variant.\nLink\n\n\n\n\n\n\n\n\n\nPlacement Optimization in Refugee ResettlementOperations Research (2021) 69(5):1468-1486\n\n\n\n\n\nwith Andrew C. Trapp, Alexander Teytelboym, Tommy Andersson, and Narges Ahani\nIn the media: Financial Times, The Atlantic, Dagens Nyheter (Swedish), SVT (Swedish)\nAbstract Every year thousands of refugees are resettled to dozens of host countries. While there is growing evidence that the initial placement of refugee families profoundly affects their lifetime outcomes, there have been few attempts to optimize resettlement destinations. We integrate machine learning and integer optimization technologies into an innovative software tool that assists a resettlement agency in the United States with matching refugees to their initial placements. Our software suggests optimal placements while giving substantial autonomy for the resettlement staff to fine-tune recommended matches. Initial back-testing indicates that Annie can improve short-run employment outcomes by 22%-37%. We discuss several directions for future work such as incorporating multiple objectives from additional integration outcomes, dealing with equity concerns, evaluating potential new locations for resettlement, managing quota in a dynamic fashion, and eliciting refugee preferences.\nLink\n\n\n\n\n\n\n\n\n\nLong-Run Saving Dynamics: Evidence from Unexpected InheritancesThe Review of Economics and Statistics (2020)\n\n\n\n\n\nwith Jeppe Druedahl\nAbstract This paper makes two contributions to the consumption literature. First, we exploit inheritance episodes to provide novel causal evidence on the long-run effects of a large financial windfall on saving behavior. For identification, we combine a longitudinal panel of administrative wealth reports with variation in the timing of sudden, unexpected parental deaths. We show that after inheritance net worth converges towards the path established before parental death, with only a third of the initial windfall remaining after a decade. These dynamics are qualitatively consistent with convergence to a buffer-stock target. Second, we interpret these findings through the lens of a generalized consumption-saving framework. To quantitatively replicate this behavior, life-cycle consumption models require impatient consumers and strong precautionary saving motives, with implications for the design of retirement policy and the value of social insurance. This result also holds for two-asset models, which imply a high marginal propensity to consume.\nLink, Code, Appendix\n\n\n\n\n\n\n\n\n\nMeasurement error in income and schooling, and the bias for linear estimatorsJournal of Labor Economics 35, no. 4 (2017)\n\n\n\n\n\nwith Paul Bingley\nAbstract We propose a general framework for determining the extent of measurement error bias in OLS and IV estimators of linear models, while allowing for measurement error in the validation source. We apply this method by validating Survey of Health, Ageing and Retirement in Europe (SHARE) data with Danish administrative registers. Contrary to most validation studies, we find measurement error in income is classical, once we account for imperfect validation data. We find non-classical measurement error in schooling, causing a 38 percent amplification bias in IV estimators of the returns, with important implications for the program evaluation literature.\nLink, Ungated version (pre-print)\n\n\n\n\n\n\n\n\n\nMental retirement and schooling European Economic Review, 63:292-298, 2013\n\n\n\n\n\nwith Paul Bingley\nAbstract We assess the validity of differences in eligibility ages for early and old age pension benefits as instruments for estimating the effect of retirement on cognitive functioning. Because differences in eligibility ages across country and gender are correlated with differences in years of schooling, which affect cognitive functioning at old ages, they are invalid as instruments without controlling for schooling. We show by means of simulation and a replication study that unless the model incorporates schooling, the estimated effect of retirement is negatively biased. This explains a large part of the “mental retirement” effects which have recently been found.\nLink"
  },
  {
    "objectID": "research.html#elephants-graveyard",
    "href": "research.html#elephants-graveyard",
    "title": "Research",
    "section": "Elephants’ graveyard",
    "text": "Elephants’ graveyard\n\n\n\n\n\n\nList over what once were working papers.\n\n\n\n\n\nUncertainty and the real economy: Evidence from Denmark\nwith Mikkel Bess, Erik Grenestam, and Jesper Pedersen\nAbstract In the media: Børsen (Danish)\nWe construct an index for economic policy uncertainty in Denmark using articles in Denmark's largest financial newspaper. We adapt a Latent Dirichlet Allocation (LDA) model to sort articles into topics. We combine article-specific topic weights with the occurrence of words describing uncertainty to construct our index. We then incorporate the index in a structural VAR model of the Danish economy and find that uncertainty is a robust predictor of investments and employment. According to our model, increased uncertainty contributed significantly to the drop in investments during the Sovereign Debt Crisis. Conversely, uncertainty has so far had a smaller, but still significant, impact on investments during the COVID-19 pandemic.\nSeeing Through the Spin: The Effect of News Sentiment on Firms' Stock Market Performance\nwith Stine Louise Daetz, Anna Kirstine Hvid, and Rastin Matin\nAbstract\nThe sentiment of news predicts the short-term stock market performance of individual companies. We find that this association is solely due to the idiosyncratic informational content of an article. We transparently quantify the association between news sentiment and stock market performance of S&P 500 companies, using articles written by Reuters between 2000 and 2018. First, we isolate the effect of sentiment independently of idiosyncratic informational content by exploiting a topic-based shift-share instrument. Second, we show that exogenous variation in article sentiment isolated through our topic-based shiftshare instrument, while strongly related to article sentiment, is unrelated to abnormal returns in the stock market.\nThe Effects of Schooling on Wealth Accumulation Approaching Retirement\nwith Paul Bingley\nAbstract\nEducation and wealth are positively correlated for individuals approaching retirement, but the direction of the causal relationship is ambiguous in theory and has not been identified in practice. We combine administrative data on individual total wealth with a reform expanding access to lower secondary school in Denmark in the 1950s, finding that schooling increases pension annuity claims but reduces the non-pension wealth of men in their 50's. These effects grow stronger as normal retirement age approaches. Labour market mechanisms are key, with schooling reducing self-employment, increasing job mobility and employment in the public sector, and improving occupational pension benefits.\nDynamic Refugee Matching\nwith Tommy Andersson and Lars Ehlers\nAbstract\nOnly one percent of the 17.2 million asylum seekers in 2016 was part of international resettlement programs: The remaining 99 percent arrived directly to their host countries without assistance from resettlement agencies. These asylum seekers are assigned to a locality directly upon arrival based on some type of dynamic matching system, which is often uninformed and does not take the background of the asylum seekers into consideration. This paper proposes an informed, intuitive, easy-to-implement and computationally efficient dynamic mechanism for matching asylum seekers to localities. This mechanism can be adopted in any dynamic refugee matching problem given locality-specific quotas and that asylum seekers can be classified into specific types. We demonstrate that any matching selected by the proposed mechanism is Pareto efficient and that envy between localities is bounded by a single asylum seeker. We evaluate the performance of the proposed mechanism in settings resembling the US and the Swedish situations, and show that our mechanism outperforms uninformed mechanisms even in presence of severe misclassification error in the estimation of asylum seeker types. With realistic misclassification error (24 percent), the proposed matching mechanism increases efficiency up to 75 percent, and guarantees a reduction in envy of between 17 and 50 percent.\nDoes Liquidity Substitute for Unemployment Insurance? Evidence from the Introduction of Home Equity Loans in Denmark\nwith Kristoffer Marwardt and László Sandór\nAbstract\nWould the value of unemployment insurance fall if more people had a buffer stock of liquid savings? Using quasi-experimental evidence from the unexpected introduction of home equity loans in Denmark, where public unemployment insurance is voluntary, we find that liquidity and insurance are substitutes. A Danish reform provided less levered homeowners with more liquidity. Using a ten-year-long panel dataset drawn from administrative registries, we find that people who obtained access to extra liquidity were less likely to sign up for unemployment insurance. The effect is concentrated among those for whom insurance has negative expected value. In this group, extra liquidity from housing equity worth one year’s income decreases insurance up-take by as much as a 0.3 percentage point fall in the risk of unemployment. Placebo tests for earlier years show no differential trends by leverage before the natural experiment. This implies that the liquidity of financial assets influences unemployment insurance uptake in the absence of public provision of insurance."
  },
  {
    "objectID": "posts/2019-11-29_gender_gap.html",
    "href": "posts/2019-11-29_gender_gap.html",
    "title": "How NOT to calculate gender gaps in wages",
    "section": "",
    "text": "Gender gaps in wages are a hugely debated topic of discussion, but often descriptive statistics are used incorrectly\nThroughout the world, women earn less than men. In Denmark, hourly wages for private secot employees are about 10% lower for women (Figure 1). There is plenty of world-class labor-economics research trying to explain why this happens. However, in the public arena, most dicussions revolve around a single misguided attempt at analyzing the sources of the gap: Adjusting for endogenous controls.\nA typical mistake in assessing gender gaps in earnings is trying to adjust it for endogenous variables, like job title (and type). This mistake is commonly made in public debates, with various pundits claiming that the actual gender gap is much lower that what people report, because we need to compare men and women working in the same position.\nThis argument is persuasive at a superficial level: that in order to assess whether discrimination actually takes place in the labor market we should compare men and women everything else equal makes sense, intuitively. In reality, assessing the extent of discrimination and quantifying this effect is extremely challenging and complex. This small note does not aim at showing how we can detect discrimination in the labor market. I would refer to a course in advanced labor economics for that. More modestly, this note aims at using an elementary example to debunk this common mistake."
  },
  {
    "objectID": "posts/2019-11-29_gender_gap.html#genesys-and-lo-men-and-women-were-created-equally",
    "href": "posts/2019-11-29_gender_gap.html#genesys-and-lo-men-and-women-were-created-equally",
    "title": "How NOT to calculate gender gaps in wages",
    "section": "Genesys: And Lo, Men and Women Were Created Equally",
    "text": "Genesys: And Lo, Men and Women Were Created Equally\nLet’s take the extremely simple example of ten working men and ten working women, with the exact same characteristics. Each of the ten men has a level of ability or skill ranging from 1 to 10. Women are generated in exactly the same way. We are in other words in a situation of perfect equality in abilities and skills.\n\n\nCode\n# Simulate\nimport numpy as np\nimport pandas as pd\n\ndata = {\n    'ability': list(range(1,11))*2,\n    'gender': ['male']*10 + ['female']*10, \n}\n\ndata = pd.DataFrame(data)\ndata\n\n\n\n\n\n\n\n\n\nability\ngender\n\n\n\n\n0\n1\nmale\n\n\n1\n2\nmale\n\n\n2\n3\nmale\n\n\n3\n4\nmale\n\n\n4\n5\nmale\n\n\n5\n6\nmale\n\n\n6\n7\nmale\n\n\n7\n8\nmale\n\n\n8\n9\nmale\n\n\n9\n10\nmale\n\n\n10\n1\nfemale\n\n\n11\n2\nfemale\n\n\n12\n3\nfemale\n\n\n13\n4\nfemale\n\n\n14\n5\nfemale\n\n\n15\n6\nfemale\n\n\n16\n7\nfemale\n\n\n17\n8\nfemale\n\n\n18\n9\nfemale\n\n\n19\n10\nfemale"
  },
  {
    "objectID": "posts/2019-11-29_gender_gap.html#the-fall-from-eden-promotions-and-wages",
    "href": "posts/2019-11-29_gender_gap.html#the-fall-from-eden-promotions-and-wages",
    "title": "How NOT to calculate gender gaps in wages",
    "section": "The Fall From Eden: Promotions and Wages",
    "text": "The Fall From Eden: Promotions and Wages\nAt this stage, I decide to promote some of my workers to managerial positions. Obviously anyone with a skill level above 8 will be promoted, these are excellent people, whether they are men or women. However, I need more managers, and for whatever reason (they posture more, they ask for a promotion, they are my drinking buddies) I decide to fill up the remaining managerial positions only with the best of the remaining (mediocre) men.\nThen I set wages. As a baseline, I decide to give people a wage equal to twice their ability level if they are managers, and a wage equal their ability level if they are not managers. However, I am also discriminating in wages. So I cut the wage of each woman by 10%.\nIn other words, I am discriminating against women in two separate ways.\n\nI do not promote them (ah, the glass ceiling!).\nI pay them less.\n\n\n\nCode\ndata.loc[:, 'manager'] = (data.loc[:, 'ability']&gt;8) | ((data.loc[:, 'gender']=='male') & (data.loc[:, 'ability']&gt;5))\n\ndata[\"wages\"] = 0.0\ndata.loc[:, 'wages'] = data.loc[:, 'ability'] + data.loc[:, 'ability']*data.loc[:, 'manager']\ndata.loc[:, 'wages'] = data.loc[:, 'wages'] - 0.1*(data.loc[:, 'gender']=='female')*data.loc[:, 'wages']\n\ndata\n\n\n/tmp/ipykernel_11182/1262971268.py:4: FutureWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n  data.loc[:, 'wages'] = data.loc[:, 'ability'] + data.loc[:, 'ability']*data.loc[:, 'manager']\n\n\n\n\n\n\n\n\n\nability\ngender\nmanager\nwages\n\n\n\n\n0\n1\nmale\nFalse\n1.0\n\n\n1\n2\nmale\nFalse\n2.0\n\n\n2\n3\nmale\nFalse\n3.0\n\n\n3\n4\nmale\nFalse\n4.0\n\n\n4\n5\nmale\nFalse\n5.0\n\n\n5\n6\nmale\nTrue\n12.0\n\n\n6\n7\nmale\nTrue\n14.0\n\n\n7\n8\nmale\nTrue\n16.0\n\n\n8\n9\nmale\nTrue\n18.0\n\n\n9\n10\nmale\nTrue\n20.0\n\n\n10\n1\nfemale\nFalse\n0.9\n\n\n11\n2\nfemale\nFalse\n1.8\n\n\n12\n3\nfemale\nFalse\n2.7\n\n\n13\n4\nfemale\nFalse\n3.6\n\n\n14\n5\nfemale\nFalse\n4.5\n\n\n15\n6\nfemale\nFalse\n5.4\n\n\n16\n7\nfemale\nFalse\n6.3\n\n\n17\n8\nfemale\nFalse\n7.2\n\n\n18\n9\nfemale\nTrue\n16.2\n\n\n19\n10\nfemale\nTrue\n18.0"
  },
  {
    "objectID": "posts/2019-11-29_gender_gap.html#sec-honest-gg",
    "href": "posts/2019-11-29_gender_gap.html#sec-honest-gg",
    "title": "How NOT to calculate gender gaps in wages",
    "section": "Honest Gender Gaps",
    "text": "Honest Gender Gaps\n\n\nCode\nhonest_gg = data.groupby('gender').mean()\nhgg = (\n  honest_gg.loc[\"female\", \"wages\"] - honest_gg.loc[\"male\", \"wages\"]\n  )/honest_gg.loc[\"male\", \"wages\"]\n\n\nWe all agree that there is a sizable amount of gender discrimination in this example. This staggering amount of discrimination should be detectable in my simulated wages. And rightly enough, the raw gender gap between genders is -29.89% – that is, women earn about 30% less than men. This makes sense, as this gap represents the 10% pay cut I gave to each woman, plus an additional component driven by the discrimination in promotions."
  },
  {
    "objectID": "posts/2019-11-29_gender_gap.html#adjusted-gender-gaps",
    "href": "posts/2019-11-29_gender_gap.html#adjusted-gender-gaps",
    "title": "How NOT to calculate gender gaps in wages",
    "section": "“Adjusted” Gender Gaps",
    "text": "“Adjusted” Gender Gaps\n\nWait wait wait wait wait-a-minute!\nSays the clever pundit dude.\nCertainly you are making a naïve mistake! Certainly you can’t compare pears and apples! These people are doing different jobs. Certainly you should compare the gender gap within an occupation to obtain the true gender gap!\n\nWell… Let’s see what happens if we compare gender gaps within occupation titles.\n\n\nCode\nadj_gg = data.groupby(['gender', 'manager'])[\"wages\"].mean().reset_index().pivot(columns='gender', index='manager')\nadj_gg[\"Gender gap\"] = (\n  adj_gg.iloc[:, 0]-adj_gg.iloc[:, 1]\n)/adj_gg.iloc[:, 1]\n\nmanager_gg = adj_gg.loc[True, \"Gender gap\"].values[0]\nnmanager_gg = adj_gg.loc[False, \"Gender gap\"].values[0]\nadj_gg\n\n\n\n\n\n\n\n\n\nwages\nGender gap\n\n\ngender\nfemale\nmale\n\n\n\nmanager\n\n\n\n\n\n\n\nFalse\n4.05\n3.0\n0.35000\n\n\nTrue\n17.10\n16.0\n0.06875\n\n\n\n\n\n\n\nWhoa! Here it shows that are actually men being discriminated against! Women make more than men on average in each role. Like, among non-managers, females make a staggering 35% more than men! And even among managers women outearn the men by almost 7%!\n\nSee?\nSays the clever pundit dude\nIt’s men who need protection! We are cuddling women too much and now they exploit it and outearn men and men feel sad ☹️"
  },
  {
    "objectID": "posts/2019-11-29_gender_gap.html#adjusting-for-endogenous-variable-will-bias-your-results",
    "href": "posts/2019-11-29_gender_gap.html#adjusting-for-endogenous-variable-will-bias-your-results",
    "title": "How NOT to calculate gender gaps in wages",
    "section": "Adjusting for endogenous variable will bias your results",
    "text": "Adjusting for endogenous variable will bias your results\nSo, how is are these results possible when I was discriminating like a maniac just a few lines above? Well, that’s because we are controlling for an endogenous variable, that is, job title. Because whether you are a manager or not depends not only on your ability but also on your gender, this control introduces a subtle type of bias.\n\n\n\n\n\n\n\n\n\nflowchart\n  Gender[Gender] ==&gt; Title(Job title)\n  Gender ==&gt; Wage\n  Ability[Ability] -.-&gt; Title\n  Title ==&gt; Wage(Wage)\n  Ability -.-&gt; Wage\n  linkStyle 0,1,3 stroke:green\n  style Gender fill:#a7f5b4\n  style Wage fill:#ff8f87\n\n\n Regressing wage on gender identifies the gross effect, i.e. the green flow… \n\n\n\n\n\n\n\n\nflowchart\n  Gender[Gender] ==&gt; Title(Job title)\n  Gender ==&gt; Wage\n  Ability[Ability] ==&gt; Title\n  Title --&gt; Wage(Wage)\n  Ability ==&gt; Wage\n  linkStyle 1 stroke:green\n  linkStyle 0,2,4 stroke:red\n  style Gender fill:#a7f5b4\n  style Wage fill:#ff8f87\n  style Title fill:#8ba9fc\n\n\n … but adjusting for job title opens a back-door path through ability (red) \n\n\n\n\n\n\nFigure 2: A Directed Acyclic Graph (DAG) allows to visualize why the bias occurrs if we adjust for job title, but not ability\n\n\n\nFigure 2 visualizes the causal relationships in our simulated example. Gender and ability both affect job title and wages. Job title in turn affects wages as well. While ability is inherently unobservable (there are multiple dimensions of ability), job title, wage, and gender are typically observable.\nThe right panel shows that if we regress wage on gender, i.e. we compute the honest gender gap as in Section 3, what we are actually identify is the full flow of effects going from gender to wage, highlighted in green in the left panel of Figure 2. That’s great! In our example, that’s the direct wage penalty of 10% plus the indirect effect from lower promotion rates.\nWhat happens however once we adjust for job title (shown in blue in the right panel)? Well, then we are blocking the effect going from job title to wages, sure. However, we are also opening a back-door path through ability (shown in red). So the total effect we end up estimating is the direct 10% effect, plus a bias component resulting from the fact that the abilities of workers within each job title–and thereby wages–will depend on gender. Because we are discriminating when promoting.\nConfused yet? Let’s look at what happens to the abilities of workers for each job title.\n\n\nCode\nab_gap = data.groupby(['gender', 'manager'])[\"ability\"].mean().reset_index().pivot(columns='gender', index='manager')\nab_gap\n\n\n\n\n\n\n\n\n\nability\n\n\ngender\nfemale\nmale\n\n\nmanager\n\n\n\n\n\n\nFalse\n4.5\n3.0\n\n\nTrue\n9.5\n8.0\n\n\n\n\n\n\n\nDespite men and women being on average absolutely identical in terms of ability, due to our selection and dicrimination into job titles, we get the apparently paradoxical result that women on average outsmart men within each occupation type. And because ability does matter for wages after all, we get a bias in out gender gap estimates: It’s natural that, on average, within each occupation group, women should earn more.\n\n\n\n\n\n\nAdjusting for something can be worse than not adjusting at all\n\n\n\nLet me stress this point, as it can appear counterintuitive at first. You might think that, intuitively, the more you adjust for stuff, the closest you get to a pure effect.\nNope. If you do not know what you are doing, and do not have a clear mental causal model, blindly adjusting for covariates in a kitchen-sink approach risks doing more harm than good."
  },
  {
    "objectID": "posts/2019-11-29_gender_gap.html#summing-up",
    "href": "posts/2019-11-29_gender_gap.html#summing-up",
    "title": "How NOT to calculate gender gaps in wages",
    "section": "Summing up",
    "text": "Summing up\nWhat should then be the proper measure of gender gap? Well, consider that we have 10 individual of each gender, each of them a doppelganger in terms of ability of each other. So for each doppelganger couple \\(j\\in{1,\\ldots,10}\\) we could compare the wage of each woman $ w^{f}_{j}$ with that of her male doppelganger \\(w^{m}_{j}\\), take the difference in wages, and average these numbers:\n\\[\n\\frac{1}{10}\\sum_{j=1}^{10}(w^{m}_{j} - w^{f}_{j}) = \\frac{1}{10}\\sum_{j=1}^{10}w^{m}_{j} - \\frac{1}{10}\\sum_{j=1}^{10}w^{f}_{j}\n\\]\nwhich is equal to, guess what, the raw difference in average wage -29.89%.\nThe bottom line is that quantifying discrimination (by gender, race, whatnot) in the labor market is hard. Terribly so.\nInterpreting comparisons other than raw gender gaps takes training and expertise, and in most cases such comparisons just muddy the waters and make gender gaps less interpretable. The raw gender gap is instead a very simple yet powerful statistics. It’s easily interpretable, and while everyone agrees it has multiple causes, it’s still highly informative.\nSo either prepare to discuss your detailed set of assumptions (for example through a DAG) or stick to that."
  },
  {
    "objectID": "tidbits.html",
    "href": "tidbits.html",
    "title": "Tidbits",
    "section": "",
    "text": "A collection of thoughts on data, statistics, and anything else really.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFive focus points for AI governance\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nApr 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nUsing DST’s API from python\n\n\n\n\n\n\ndata tools\n\n\n\nLearn how to extract data from Danmark Statistiks Statistikbanken with python\n\n\n\n\n\nFeb 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nHow NOT to calculate gender gaps in wages\n\n\n\n\n\n\nanalytics\n\n\n\nDo not adjust for job type and title. Just don’t.\n\n\n\n\n\nNov 29, 2019\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alessandro Tang-Andersen Martinello",
    "section": "",
    "text": "Originally from Jesolo, I obtained my PhD in Economics at the University of Copenhagen, and have worked both in academia at Lund University and in policy at the Danish Central Bank.\nI am now Head of Data and Analytics at Realkredit Danmark, part of Danske Bank group.\nHere you can find both my (very infrequenctly updated) blog and my old research projects."
  },
  {
    "objectID": "posts/2022-04-25_AI_5_focus_points.html",
    "href": "posts/2022-04-25_AI_5_focus_points.html",
    "title": "Five focus points for AI governance",
    "section": "",
    "text": "You might know that the European Commission has submitted a proposal for an AI act.\nThe proposal defines high-risk AI systems, requires minimum governance structures for such systems, and imposes large fines for lack of compliance, up to 6% of a firm’s global turnover.\nThe proposal as it stands will have consequences for the financial sector. Among high-risk systems, the act specifically includes systems for credit risk scores applied to natural persons - not only those using machine learning, but also those using standard statistical tools (logit, OLS, whatnot).\nAt Danmarks Nationalbank we have studied both the academic and non-academic literature, and consulted with national and international experts to understand the challenges and best practices in using AI in the financial sector. While machine learning is not a new paradigm for banks and financial institutions, who are used to exploit microdata and models to inform their business, it can amplify existing risks.\nIn this memo we summarize our findings, and suggest five focus points for financial institution to consider when moving from static statistical models to self-learning, dynamic AI systems.\n\n\n\nFive focus points for AI governance\n\n\nHave a read, and if you have any input, let us know!"
  },
  {
    "objectID": "posts/Using DSTs API with python.html",
    "href": "posts/Using DSTs API with python.html",
    "title": "Using DST’s API from python",
    "section": "",
    "text": "I am a huge fan of Denmark Statistics. Their Statistikbanken contains a wealth of data on the Danish society, economy, and population.\nNot only all these data are publicy available, but DST has for years also provided access to all their published data tables through an API, documented here. The API access makes it extremely easy to access and use data. Yet unless one has already some experience in accessing APIs, using it might be complex for an occasional student or analyst.\nThis notebook provides a quick guide on how to access data from DST’s Statsbanken through their API, and presents a utility class I wrote to more easily access data tables for analytical purposes.\nThe only explicit dependency of that utility is pandas, which is anyway an extremely widespread package.\nBoth notebook and class can be found at this GitHub repository.\nThe utility can be installed by\n# Start by importing necessary packages\nimport requests\nimport pandas as pd\nfrom IPython.display import display\nfrom io import StringIO\n\nfrom dstapi import DstApi # The helper class\nDST’s API has four separate function calls to programmatically navigate around the published tables. This guide assumes that the analysist has scouted Statistikbanken already, and has identified the one or two tables from which data should be extracted. For these purposes, we only need two function calls: tableinfo and data.\nThe standard process is to begin by obtaining the necessary information from tableinfo, and then construct the call to pass to data.\nThis guide will proceed by for each step of the process first showing how to do it by directly using requests (and pandas), and second showing how the utility class DstApi can facilitate the process."
  },
  {
    "objectID": "posts/Using DSTs API with python.html#step-1-understand-what-a-table-has-to-offer-and-how-it-is-structured",
    "href": "posts/Using DSTs API with python.html#step-1-understand-what-a-table-has-to-offer-and-how-it-is-structured",
    "title": "Using DST’s API from python",
    "section": "Step 1: Understand what a table has to offer and how it is structured",
    "text": "Step 1: Understand what a table has to offer and how it is structured\nOur primary example will be DST’s table METROX1, which reports an index measuring the weekly amount of passengers travelling by metro in Copenhagen. This index was developed to measure the population’s response to the COVID pandemic. The table is small and simple, allowing for quick experimentation.\n\nThe hard way\nAs we know the table’s name/id we can start by accessing the API directly through the python package requests, and ask about the table’s metadata (tableinfo).\nAn API call is composed by a main web address, a function call, and a set of parameters. The main web address is https://api.statbank.dk/v1. The function call in this case is tableinfo. The set of necessary parameters, per documentation, is the id of the table and the format in which we’d like to receive the information. We’ll pick \"metrox1\" for the first (note that the table-id parameter is case-sensitive), and \"JSON\" for the second.\nThe API at DST can be called through both requests.get() and requests.post(). DST’s documentation recommends using post, because as the number and complexity of parameters grows (with some of them containing non-standard Danish characters) it’s harder to embed them in an URL. However, as the call to tableinfo is simple, below I provide examples of using both methods.\nNote that the .json() method of the request.Response object serves to return the response content (which we requested in JSON format) rather than the object itself. That’s just to print out the output in the notebook.\nThis function returns a wealth of information. Not just the table id and description, but also the contact of the statistics responsible, and, crucially, names and values of the variables defining the table. In this case SÆSON and Tid.\nThe code below shows how to get the table’s metadata, and prints the beginning of the JSON file returned.\n\n\nShow the code\n# Directly embed parameters in the URL with response.get()\nrequests.get('https://api.statbank.dk/v1' + '/tableinfo' + \"?id=metrox1&format=JSON\").json()\n\n# Pass a dictionary of parameters to requests.get()\nparams = {'id': 'metrox1', 'format': 'JSON'}\nrequests.get('https://api.statbank.dk/v1' + '/tableinfo', params=params).json()\n\n# Use response.post() - note the change in the name of the parameter about the table's name\n# I'm also adding here a language parameter - most tables are available in both Danish and English\nparams = {'table': 'metrox1', 'format': 'JSON', 'lang':'en'}\ntable_metadata = requests.post(\n    'https://api.statbank.dk/v1' + '/tableinfo', json=params\n).json()\nprint(str(table_metadata).replace(',', ',\\n')[0:500] + '\\n...')\n\n\n{'id': 'METROX1',\n 'text': 'Workday passenger index in the Copenhagen Metro (experimental statistics)',\n 'description': 'Workday passenger index in the Copenhagen Metro (experimental statistics) by seasonal adjustment and time',\n 'unit': 'Index',\n 'suppressedDataValue': '0',\n 'updated': '2022-06-16T08:00:00',\n 'active': False,\n 'contacts': [{'name': 'Peter Ottosen',\n 'phone': '+4530429191',\n 'mail': 'pot@dst.dk'}],\n 'documentation': None,\n 'footnote': {'text': 'Data are indexed against an averag\n...\n\n\nThis wealth of information is already fantastic. In that metadata there’s pretty much anything you need to figure out if you can actually use the table, and eventually how you want to select the data (seasonally ajusted? For 2020 only?). Yet that JSON file might be tough to digest, especially for more complex tables. Those cases might require preprocessing and a different type of visualization. That’s where the DstApi helper class comes into play.\n\n\nThe easy way\nDstApi has two methods for examining metadata.\nThe first one, tablesummary, summarizes the main metadata information: * The id and description of the table * The last update time * A table with the main available cuts of the data. Each row corresponds to a variable against which we can select, with examples of variable values and labels\n\n\nShow the code\n# Initialize the class with the target table\nmetro = DstApi('METROX1')\n\n# Get the table summary\nmetro.tablesummary(language='en')\n\n\nTable METROX1: Workday passenger index in the Copenhagen Metro (experimental statistics) by seasonal adjustment and time\nLast update: 2022-06-16T08:00:00\n\n\n\n\n\n\n\n\n\nvariable name\n# values\nFirst value\nFirst value label\nLast value\nLast value label\nTime variable\n\n\n\n\n0\nSÆSON\n2\n10\nSeasonally adjusted\n11\nNon-seasonally adjusted\nFalse\n\n\n1\nTid\n122\n2020U01\n2020U01\n2022U23\n2022U23\nTrue\n\n\n\n\n\n\n\nThe second method variable_levels zooms into a specific variable and returns a dataframe for each potential variable value. For example, we could check each value of SÆSON\n\n\nShow the code\nmetro.variable_levels('SÆSON', language='en')\n\n\n\n\n\n\n\n\n\nid\ntext\n\n\n\n\n0\n10\nSeasonally adjusted\n\n\n1\n11\nNon-seasonally adjusted\n\n\n\n\n\n\n\nNow, we already knew these values for this simple table from tablesummary(), as they are only two. But for more complex tables, this method is very handy. Take for example DNVPDKR2, a table showing the circulating amount of mortgage bonds issued by Danish mortgage institutes.\nIf I wanted for example to extract only data about fixed interest rate, convertible bonds it would be hard to know I should be referring to the value FK in advance.\nBut I can use\n\nmethod .tablesummary() to see which variables you can select on\nmethod .variable_levels() to see which values are available for each variable. Here * is a wildcard that selects all available values for the variable.\n\n\n\nShow the code\ndnrk = DstApi('DNVPDKR2')\ndnrk.tablesummary(language='en')\n\n\nTable DNVPDKR2: Danish mortgage bonds by type of mortgage bond, original maturity, remaining maturity, coupon (nominal interest rate), currency, issuer, investor sector, covered bonds, data type and time\nLast update: 2024-11-28T08:00:00\n\n\n\n\n\n\n\n\n\nvariable name\n# values\nFirst value\nFirst value label\nLast value\nLast value label\nTime variable\n\n\n\n\n0\nTYPREAL\n9\nA0\nAll mortgage bonds\nO\n1.6 Other mortgage bonds\nFalse\n\n\n1\nLØBETID3\n7\nA0\nAll original maturities\n6\nOther (unspecified)\nFalse\n\n\n2\nLØBETID2\n9\nA0\nAll remaining maturities\n8\nOther (unspecified)\nFalse\n\n\n3\nKUPON2\n15\nA0\nAll coupons\nN\nOther coupons\nFalse\n\n\n4\nVALUTA\n6\nA0\nAll currencies\nO\nOther\nFalse\n\n\n5\nUDSTED\n10\nA0\nAll issuers\nO\nOther issuers\nFalse\n\n\n6\nINVSEKTOR\n10\nA0\nAll sectors\nU\n2. Foreign (S.2)\nFalse\n\n\n7\nDAEKOBL\n4\nA0\nAll mortgage bonds\nRO\nRO (other mortgage bonds)\nFalse\n\n\n8\nDATAT\n5\nN1\nStock - Nominal\nB3\nValue adjustments - Market value\nFalse\n\n\n9\nTid\n299\n1999M12\n1999M12\n2024M10\n2024M10\nTrue\n\n\n\n\n\n\n\n\n\nShow the code\ndnrk.variable_levels('TYPREAL', language='en')\n\n\n\n\n\n\n\n\n\nid\ntext\n\n\n\n\n0\nA0\nAll mortgage bonds\n\n\n1\nFK\n1.1 Fixed rate convertible bonds\n\n\n2\nFKU\n- - 1.1.1 Open for issue - fixed rate converti...\n\n\n3\nFKE\n- - 1.1.2 No longer open for issue - fixed rat...\n\n\n4\nI\n1.2 Indexed bonds\n\n\n5\nRTL\n1.3 Adjustable rate bonds (RTL bonds)\n\n\n6\nV\n1.4 Bonds with a rererence rate (without inter...\n\n\n7\nVR\n1.5 Bonds with a reference rate (with interest...\n\n\n8\nO\n1.6 Other mortgage bonds"
  },
  {
    "objectID": "posts/Using DSTs API with python.html#step-2-get-the-data-you-need",
    "href": "posts/Using DSTs API with python.html#step-2-get-the-data-you-need",
    "title": "Using DST’s API from python",
    "section": "Step 2: Get the data you need",
    "text": "Step 2: Get the data you need\nThe first step is essential for designing this second step. First and foremost because we need that information to design the call to data. Second, to make sure we only get out the data we need. Asking for too much data only to then having to throw half of it out locally is wasteful, and ultimately disrespectful for the resources invested into allowing anyone to fire up an API call (I mean how amazing is that?).\n\nThe hard way\nAs for the first step, we’ll start by doing it manually. Here I’ll rely exclusively on request.post() as recommended by DST.\nTo select the query parameters to pass to the data function appropriatedly one ought to have a careful look at the DATA section in the documentation. Nonetheless, hopefully the examples below will serve to clarify how to construct such calls.\nThe first two key parameters are, as before, the table name and the format in which we’d like to obtain the data. In the examples below I choose BULK, which has the advantage of being faster and allowing an unlimited number of data rows at export. There are some limitations with this format, such as the inability to perform simple computations (e.g. sums) on the fly. If you need these utilities, you probably don’t need this guide, so I’ll stick with BULK here.\nThe third crucial parameter is the selection based on the variables shown in e.g. DstApi.tablesummary(). These are mandatory: we need to specify the selection we want to do. We might however choose to include a range of possible values, or all of them, in a selection. In this case, the character * acts as a joker. So to select all values of a variable, we can use *. To select all 2020 weeks in Tid, we could use 2020*.\nBelow I write the parameters necessary to download the seasonally adjusted (code 10) index for all weeks in the data, and pass them to requests.post(). Finally I print the first 200 characters of the data we received back (in ;-separated format).\n\nparams = {\n    'table': 'metrox1',\n    'format': 'BULK',\n    'variables': [\n        {'code': 'SÆSON', 'values': ['10']},\n        {'code': 'Tid', 'values': ['*']}\n    ]\n}\nr = requests.post('https://api.statbank.dk/v1' + '/data', json=params)\nprint(r.text[:200])\n\nSÆSON;TID;INDHOLD\nSæsonkorrigeret;2020U01;37,7\nSæsonkorrigeret;2020U08;105,0\nSæsonkorrigeret;2020U09;95,2\nSæsonkorrigeret;2020U10;93,0\nSæsonkorrigeret;2020U11;63,0\nSæsonkorrigeret;2020U12;17,9\n\n\n\nNeat! We can then save this data to a csv file or whatever, or directly import it into pandas:\n\n\nShow the code\npd.read_table(StringIO(r.text), sep=';').head()\n\n\n\n\n\n\n\n\n\nSÆSON\nTID\nINDHOLD\n\n\n\n\n0\nSæsonkorrigeret\n2020U01\n37,7\n\n\n1\nSæsonkorrigeret\n2020U08\n105,0\n\n\n2\nSæsonkorrigeret\n2020U09\n95,2\n\n\n3\nSæsonkorrigeret\n2020U10\n93,0\n\n\n4\nSæsonkorrigeret\n2020U11\n63,0\n\n\n\n\n\n\n\nKeep in mind that you can also specify intervals for time variables, as in the example below, where I also require the data to be exported in English.\n\n\nShow the code\nparams = {\n    'table': 'metrox1',\n    'format': 'BULK',\n    'lang': 'en',\n    'variables': [\n        {'code': 'SÆSON', 'values': ['11']},\n        {'code': 'Tid', 'values': ['&gt;2020U45&lt;=2020U52']}\n    ]\n}\ndf = pd.read_csv(\n    StringIO(\n        requests.post('https://api.statbank.dk/v1' + '/data', json=params).text\n    ), sep=';'\n)\ndf.head()\n\n\n\n\n\n\n\n\n\nSÆSON\nTID\nINDHOLD\n\n\n\n\n0\nNon-seasonally adjusted\n2020U46\n56.2\n\n\n1\nNon-seasonally adjusted\n2020U47\n55.5\n\n\n2\nNon-seasonally adjusted\n2020U48\n58.3\n\n\n3\nNon-seasonally adjusted\n2020U49\n57.6\n\n\n4\nNon-seasonally adjusted\n2020U50\n46.9\n\n\n\n\n\n\n\n\n\nThe easy-er way\nThe code above is already quite compact, but to avoid remembering how to import the data into pandas all the time, DstApi has a method to import the data directly into pandas given a parameter dictionary. So, for example, given the params dictionary defined above, we might call directly\n\n\nShow the code\nmetro.get_data(params=params)\n\n\n\n\n\n\n\n\n\nSÆSON\nTID\nINDHOLD\n\n\n\n\n0\nSæsonkorrigeret\n2020U01\n37.7\n\n\n1\nSæsonkorrigeret\n2020U08\n105.0\n\n\n2\nSæsonkorrigeret\n2020U09\n95.2\n\n\n3\nSæsonkorrigeret\n2020U10\n93.0\n\n\n4\nSæsonkorrigeret\n2020U11\n63.0\n\n\n...\n...\n...\n...\n\n\n117\nSæsonkorrigeret\n2022U19\n99.5\n\n\n118\nSæsonkorrigeret\n2022U20\n95.7\n\n\n119\nSæsonkorrigeret\n2022U21\n103.1\n\n\n120\nSæsonkorrigeret\n2022U22\n108.4\n\n\n121\nSæsonkorrigeret\n2022U23\n109.3\n\n\n\n\n122 rows × 3 columns\n\n\n\nthe .get_data() method has also the built-in option of downloading an entire data table by not passing any parameter dictionary. As mentioned above, this might be (very) wasteful. Some DST tables contain billions of data points. That’s why when used in this way the method asks for explicit confirmation before proceeding.\nHowever, creating the params dictionary itself can be challenging. As we have seen above with table DNVPDKR2, table structures can be complex, and creating the parameter dictionary manually can be cumbersome.\nThat’s why DstApi has a helper method returning a base dictionary of parameters with default values.\n\n\n\n\n\n\nOnly pass a default params to .get_data() if you know what you are doing\n\n\n\nSome tables in Statistikbanken have millions of records. Downloading them all through the api can take a lot of time, and it’s extremely wasteful if in fact you only need a fraction of the data.\n\n\n\n\nShow the code\n# Start by constructing a basic dictionary\ndnrk = DstApi('DNVPDKR2')\nparams = dnrk.define_base_params(language = 'en')\nparams\n\n\n{'table': 'dnvpdkr2',\n 'format': 'BULK',\n 'lang': 'en',\n 'variables': [{'code': 'TYPREAL', 'values': ['*']},\n  {'code': 'LØBETID3', 'values': ['*']},\n  {'code': 'LØBETID2', 'values': ['*']},\n  {'code': 'KUPON2', 'values': ['*']},\n  {'code': 'VALUTA', 'values': ['*']},\n  {'code': 'UDSTED', 'values': ['*']},\n  {'code': 'INVSEKTOR', 'values': ['*']},\n  {'code': 'DAEKOBL', 'values': ['*']},\n  {'code': 'DATAT', 'values': ['*']},\n  {'code': 'Tid', 'values': ['*']}]}\n\n\nOnce I have the basic structure, I can copy-paste the dictionary definition and use the method variable_levels to specify the data selection further. For example, I would like to have only bonds issued by Realkredit Danmark, so the code below tells me to use value RD for variable DAEKOBL.\n\n\nShow the code\ndnrk.variable_levels('LØBETID3', language='en')\n\n\n\n\n\n\n\n\n\nid\ntext\n\n\n\n\n0\nA0\nAlle oprindelige løbetider\n\n\n1\n1\nUnder 10 år\n\n\n2\n2\n10-årige\n\n\n3\n3\n15-årige\n\n\n4\n4\n20-årige\n\n\n5\n5\n30-årige\n\n\n6\n6\nAndet (uspecificeret)\n\n\n\n\n\n\n\nI can further refine my query filling in the selection parameters required and call the get_data() method to extract the final dataframe.\n\n\nShow the code\nparams = {'table': 'dnvpdkr2',\n 'format': 'BULK',\n 'lang': 'en',\n 'variables': [{'code': 'TYPREAL', 'values': ['FK']},\n  {'code': 'LØBETID3', 'values': ['5']},\n  {'code': 'LØBETID2', 'values': ['A0']},\n  {'code': 'KUPON2', 'values': ['A0']},\n  {'code': 'VALUTA', 'values': ['DKK']},\n  {'code': 'UDSTED', 'values': ['RD']},\n  {'code': 'INVSEKTOR', 'values': ['A0']},\n  {'code': 'DAEKOBL', 'values': ['A0']},\n  {'code': 'DATAT', 'values': ['N1']},\n  {'code': 'Tid', 'values': ['*']}]}\ndf = dnrk.get_data(params=params, language='en')\ndf.tail()[[\"TID\", \"INDHOLD\"]]\n\n\n\n\n\n\n\n\n\nTID\nINDHOLD\n\n\n\n\n294\n2024M06\n229104\n\n\n295\n2024M07\n228410\n\n\n296\n2024M08\n229411\n\n\n297\n2024M09\n231149\n\n\n298\n2024M10\n231694\n\n\n\n\n\n\n\nAnd just like that, I have the full time series of RD’s 30yo fixed interest rate bonds in nominal values.\nYou can play around with parameters in various ways. For example, here I select a range of weeks in 2020 from the metro table.\n\n\nShow the code\n# Start by constructing a basic dictionary\nparams = metro._define_base_params(language = 'en')\nparams['variables'][0]['values'] = ['10']\nparams['variables'][1]['values'] = ['&gt;2020U45&lt;=2020U52']\nmetro.get_data(params=params)\n\n\n\n\n\n\n\n\n\nSÆSON\nTID\nINDHOLD\n\n\n\n\n0\nSeasonally adjusted\n2020U46\n59.0\n\n\n1\nSeasonally adjusted\n2020U47\n54.0\n\n\n2\nSeasonally adjusted\n2020U48\n56.2\n\n\n3\nSeasonally adjusted\n2020U49\n54.5\n\n\n4\nSeasonally adjusted\n2020U50\n44.3\n\n\n5\nSeasonally adjusted\n2020U51\n40.2\n\n\n6\nSeasonally adjusted\n2020U52\n44.8"
  },
  {
    "objectID": "posts/Using DSTs API with python.html#and-thats-it",
    "href": "posts/Using DSTs API with python.html#and-thats-it",
    "title": "Using DST’s API from python",
    "section": "And that’s it!",
    "text": "And that’s it!\nI hope this guide was useful, and that the DstApi class can prove as helpful to you as it is for me.\nOnce again, let me conclude with a shout out to Denmark Statistics, a real national treasure. Thanks for all your work in gathering, organizing, and publishing data for everyone to use. It’s a fantastic service, and one for which you’ll never be thanked enough."
  }
]