[
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Over the years I have worked on many different fields, from household finance to refugee resettlement. Here you can find my published work, togeher with a long-ish list of pretty-much-done projects that were however never submitted–or submitted only once or twice before I abandoned academia. I refer to these as resting in the elephants’ graveyard."
  },
  {
    "objectID": "research.html#publications",
    "href": "research.html#publications",
    "title": "Research",
    "section": "Publications",
    "text": "Publications\n\n\n\n\n\n\nSubstantial transmission of SARS-CoV-2 through casual contact in retail stores: Evidence from matched administrative microdata on card payments and testingPNAS (2024) 121 (17)\n\n\n\n\n\nwith Niels Johannesen, Bjørn Bjørnsson Meyer, Emil Toft Vestergaard, Asger Lau Andersen, and Thais Lærkholm Jensen\nAbstract This paper presents quasiexperimental evidence of Covid-19 transmission through casual contact between customers in retail stores. For a large sample of individuals in Denmark, we match card payment data, indicating exactly where and when each individual made purchases, with Covid-19 test data, indicating when each individual was tested and whether the test was positive. The resulting dataset identifies more than 100,000 instances where an infected individual made a purchase in a store and, in each instance, allows us to track the infection dynamics of other individuals who made purchases in the same store around the same time. We estimate transmissions by comparing the infection rate of exposed customers, who made a purchase within 5 min of an infected individual, and nonexposed customers, who made a purchase in the same store 16 to 30 min before. We find that exposure to an infected individual in a store increases the infection rate by around 0.12 percentage points (P &lt; 0.001) between day 3 and day 7 after exposure. The estimates imply that transmissions in stores contributed around 0.04 to the reproduction number for the average infected individual and significantly more in the period where Omicron was the dominant variant.\nLink\n\n\n\n\n\n\n\n\n\nPlacement Optimization in Refugee ResettlementOperations Research (2021) 69(5):1468-1486\n\n\n\n\n\nwith Andrew C. Trapp, Alexander Teytelboym, Tommy Andersson, and Narges Ahani\nIn the media: Financial Times, The Atlantic, Dagens Nyheter (Swedish), SVT (Swedish)\nAbstract Every year thousands of refugees are resettled to dozens of host countries. While there is growing evidence that the initial placement of refugee families profoundly affects their lifetime outcomes, there have been few attempts to optimize resettlement destinations. We integrate machine learning and integer optimization technologies into an innovative software tool that assists a resettlement agency in the United States with matching refugees to their initial placements. Our software suggests optimal placements while giving substantial autonomy for the resettlement staff to fine-tune recommended matches. Initial back-testing indicates that Annie can improve short-run employment outcomes by 22%-37%. We discuss several directions for future work such as incorporating multiple objectives from additional integration outcomes, dealing with equity concerns, evaluating potential new locations for resettlement, managing quota in a dynamic fashion, and eliciting refugee preferences.\nLink\n\n\n\n\n\n\n\n\n\nLong-Run Saving Dynamics: Evidence from Unexpected InheritancesThe Review of Economics and Statistics (2020)\n\n\n\n\n\nwith Jeppe Druedahl\nAbstract This paper makes two contributions to the consumption literature. First, we exploit inheritance episodes to provide novel causal evidence on the long-run effects of a large financial windfall on saving behavior. For identification, we combine a longitudinal panel of administrative wealth reports with variation in the timing of sudden, unexpected parental deaths. We show that after inheritance net worth converges towards the path established before parental death, with only a third of the initial windfall remaining after a decade. These dynamics are qualitatively consistent with convergence to a buffer-stock target. Second, we interpret these findings through the lens of a generalized consumption-saving framework. To quantitatively replicate this behavior, life-cycle consumption models require impatient consumers and strong precautionary saving motives, with implications for the design of retirement policy and the value of social insurance. This result also holds for two-asset models, which imply a high marginal propensity to consume.\nLink, Code, Appendix\n\n\n\n\n\n\n\n\n\nMeasurement error in income and schooling, and the bias for linear estimatorsJournal of Labor Economics 35, no. 4 (2017)\n\n\n\n\n\nwith Paul Bingley\nAbstract We propose a general framework for determining the extent of measurement error bias in OLS and IV estimators of linear models, while allowing for measurement error in the validation source. We apply this method by validating Survey of Health, Ageing and Retirement in Europe (SHARE) data with Danish administrative registers. Contrary to most validation studies, we find measurement error in income is classical, once we account for imperfect validation data. We find non-classical measurement error in schooling, causing a 38 percent amplification bias in IV estimators of the returns, with important implications for the program evaluation literature.\nLink, Ungated version (pre-print)\n\n\n\n\n\n\n\n\n\nMental retirement and schooling European Economic Review, 63:292-298, 2013\n\n\n\n\n\nwith Paul Bingley\nAbstract We assess the validity of differences in eligibility ages for early and old age pension benefits as instruments for estimating the effect of retirement on cognitive functioning. Because differences in eligibility ages across country and gender are correlated with differences in years of schooling, which affect cognitive functioning at old ages, they are invalid as instruments without controlling for schooling. We show by means of simulation and a replication study that unless the model incorporates schooling, the estimated effect of retirement is negatively biased. This explains a large part of the “mental retirement” effects which have recently been found.\nLink"
  },
  {
    "objectID": "research.html#elephants-graveyard",
    "href": "research.html#elephants-graveyard",
    "title": "Research",
    "section": "Elephants’ graveyard",
    "text": "Elephants’ graveyard\n\n\n\n\n\n\nList over what once were working papers.\n\n\n\n\n\nUncertainty and the real economy: Evidence from Denmark\nwith Mikkel Bess, Erik Grenestam, and Jesper Pedersen\nAbstract In the media: Børsen (Danish)\nWe construct an index for economic policy uncertainty in Denmark using articles in Denmark's largest financial newspaper. We adapt a Latent Dirichlet Allocation (LDA) model to sort articles into topics. We combine article-specific topic weights with the occurrence of words describing uncertainty to construct our index. We then incorporate the index in a structural VAR model of the Danish economy and find that uncertainty is a robust predictor of investments and employment. According to our model, increased uncertainty contributed significantly to the drop in investments during the Sovereign Debt Crisis. Conversely, uncertainty has so far had a smaller, but still significant, impact on investments during the COVID-19 pandemic.\nSeeing Through the Spin: The Effect of News Sentiment on Firms' Stock Market Performance\nwith Stine Louise Daetz, Anna Kirstine Hvid, and Rastin Matin\nAbstract\nThe sentiment of news predicts the short-term stock market performance of individual companies. We find that this association is solely due to the idiosyncratic informational content of an article. We transparently quantify the association between news sentiment and stock market performance of S&P 500 companies, using articles written by Reuters between 2000 and 2018. First, we isolate the effect of sentiment independently of idiosyncratic informational content by exploiting a topic-based shift-share instrument. Second, we show that exogenous variation in article sentiment isolated through our topic-based shiftshare instrument, while strongly related to article sentiment, is unrelated to abnormal returns in the stock market.\nThe Effects of Schooling on Wealth Accumulation Approaching Retirement\nwith Paul Bingley\nAbstract\nEducation and wealth are positively correlated for individuals approaching retirement, but the direction of the causal relationship is ambiguous in theory and has not been identified in practice. We combine administrative data on individual total wealth with a reform expanding access to lower secondary school in Denmark in the 1950s, finding that schooling increases pension annuity claims but reduces the non-pension wealth of men in their 50's. These effects grow stronger as normal retirement age approaches. Labour market mechanisms are key, with schooling reducing self-employment, increasing job mobility and employment in the public sector, and improving occupational pension benefits.\nDynamic Refugee Matching\nwith Tommy Andersson and Lars Ehlers\nAbstract\nOnly one percent of the 17.2 million asylum seekers in 2016 was part of international resettlement programs: The remaining 99 percent arrived directly to their host countries without assistance from resettlement agencies. These asylum seekers are assigned to a locality directly upon arrival based on some type of dynamic matching system, which is often uninformed and does not take the background of the asylum seekers into consideration. This paper proposes an informed, intuitive, easy-to-implement and computationally efficient dynamic mechanism for matching asylum seekers to localities. This mechanism can be adopted in any dynamic refugee matching problem given locality-specific quotas and that asylum seekers can be classified into specific types. We demonstrate that any matching selected by the proposed mechanism is Pareto efficient and that envy between localities is bounded by a single asylum seeker. We evaluate the performance of the proposed mechanism in settings resembling the US and the Swedish situations, and show that our mechanism outperforms uninformed mechanisms even in presence of severe misclassification error in the estimation of asylum seeker types. With realistic misclassification error (24 percent), the proposed matching mechanism increases efficiency up to 75 percent, and guarantees a reduction in envy of between 17 and 50 percent.\nDoes Liquidity Substitute for Unemployment Insurance? Evidence from the Introduction of Home Equity Loans in Denmark\nwith Kristoffer Marwardt and László Sandór\nAbstract\nWould the value of unemployment insurance fall if more people had a buffer stock of liquid savings? Using quasi-experimental evidence from the unexpected introduction of home equity loans in Denmark, where public unemployment insurance is voluntary, we find that liquidity and insurance are substitutes. A Danish reform provided less levered homeowners with more liquidity. Using a ten-year-long panel dataset drawn from administrative registries, we find that people who obtained access to extra liquidity were less likely to sign up for unemployment insurance. The effect is concentrated among those for whom insurance has negative expected value. In this group, extra liquidity from housing equity worth one year’s income decreases insurance up-take by as much as a 0.3 percentage point fall in the risk of unemployment. Placebo tests for earlier years show no differential trends by leverage before the natural experiment. This implies that the liquidity of financial assets influences unemployment insurance uptake in the absence of public provision of insurance."
  },
  {
    "objectID": "posts/2019-11-29_gender_gap.html",
    "href": "posts/2019-11-29_gender_gap.html",
    "title": "How NOT to calculate gender gaps in wages",
    "section": "",
    "text": "Gender gaps in wages are a hugely debated topic of discussion, but often descriptive statistics are used incorrectly\nThroughout the world, women earn less than men. In Denmark, hourly wages for private secot employees are about 10% lower for women (Figure 1). There is plenty of world-class labor-economics research trying to explain why this happens. However, in the public arena, most dicussions revolve around a single misguided attempt at analyzing the sources of the gap: Adjusting for endogenous controls.\nA typical mistake in assessing gender gaps in earnings is trying to adjust it for endogenous variables, like job title (and type). This mistake is commonly made in public debates, with various pundits claiming that the actual gender gap is much lower that what people report, because we need to compare men and women working in the same position.\nThis argument is persuasive at a superficial level: that in order to assess whether discrimination actually takes place in the labor market we should compare men and women everything else equal makes sense, intuitively. In reality, assessing the extent of discrimination and quantifying this effect is extremely challenging and complex. This small note does not aim at showing how we can detect discrimination in the labor market. I would refer to a course in advanced labor economics for that. More modestly, this note aims at using an elementary example to debunk this common mistake."
  },
  {
    "objectID": "posts/2019-11-29_gender_gap.html#genesys-and-lo-men-and-women-were-created-equally",
    "href": "posts/2019-11-29_gender_gap.html#genesys-and-lo-men-and-women-were-created-equally",
    "title": "How NOT to calculate gender gaps in wages",
    "section": "Genesys: And Lo, Men and Women Were Created Equally",
    "text": "Genesys: And Lo, Men and Women Were Created Equally\nLet’s take the extremely simple example of ten working men and ten working women, with the exact same characteristics. Each of the ten men has a level of ability or skill ranging from 1 to 10. Women are generated in exactly the same way. We are in other words in a situation of perfect equality in abilities and skills.\n\n\nCode\n# Simulate\nimport numpy as np\nimport pandas as pd\n\ndata = {\n    'ability': list(range(1,11))*2,\n    'gender': ['male']*10 + ['female']*10, \n}\n\ndata = pd.DataFrame(data)\ndata\n\n\n\n\n\n\n\n\n\nability\ngender\n\n\n\n\n0\n1\nmale\n\n\n1\n2\nmale\n\n\n2\n3\nmale\n\n\n3\n4\nmale\n\n\n4\n5\nmale\n\n\n5\n6\nmale\n\n\n6\n7\nmale\n\n\n7\n8\nmale\n\n\n8\n9\nmale\n\n\n9\n10\nmale\n\n\n10\n1\nfemale\n\n\n11\n2\nfemale\n\n\n12\n3\nfemale\n\n\n13\n4\nfemale\n\n\n14\n5\nfemale\n\n\n15\n6\nfemale\n\n\n16\n7\nfemale\n\n\n17\n8\nfemale\n\n\n18\n9\nfemale\n\n\n19\n10\nfemale"
  },
  {
    "objectID": "posts/2019-11-29_gender_gap.html#the-fall-from-eden-promotions-and-wages",
    "href": "posts/2019-11-29_gender_gap.html#the-fall-from-eden-promotions-and-wages",
    "title": "How NOT to calculate gender gaps in wages",
    "section": "The Fall From Eden: Promotions and Wages",
    "text": "The Fall From Eden: Promotions and Wages\nAt this stage, I decide to promote some of my workers to managerial positions. Obviously anyone with a skill level above 8 will be promoted, these are excellent people, whether they are men or women. However, I need more managers, and for whatever reason (they posture more, they ask for a promotion, they are my drinking buddies) I decide to fill up the remaining managerial positions only with the best of the remaining (mediocre) men.\nThen I set wages. As a baseline, I decide to give people a wage equal to twice their ability level if they are managers, and a wage equal their ability level if they are not managers. However, I am also discriminating in wages. So I cut the wage of each woman by 10%.\nIn other words, I am discriminating against women in two separate ways.\n\nI do not promote them (ah, the glass ceiling!).\nI pay them less.\n\n\n\nCode\ndata.loc[:, 'manager'] = (data.loc[:, 'ability']&gt;8) | ((data.loc[:, 'gender']=='male') & (data.loc[:, 'ability']&gt;5))\n\ndata[\"wages\"] = 0.0\ndata.loc[:, 'wages'] = data.loc[:, 'ability'] + data.loc[:, 'ability']*data.loc[:, 'manager']\ndata.loc[:, 'wages'] = data.loc[:, 'wages'] - 0.1*(data.loc[:, 'gender']=='female')*data.loc[:, 'wages']\n\ndata\n\n\n\n\n\n\n\n\n\nability\ngender\nmanager\nwages\n\n\n\n\n0\n1\nmale\nFalse\n1.0\n\n\n1\n2\nmale\nFalse\n2.0\n\n\n2\n3\nmale\nFalse\n3.0\n\n\n3\n4\nmale\nFalse\n4.0\n\n\n4\n5\nmale\nFalse\n5.0\n\n\n5\n6\nmale\nTrue\n12.0\n\n\n6\n7\nmale\nTrue\n14.0\n\n\n7\n8\nmale\nTrue\n16.0\n\n\n8\n9\nmale\nTrue\n18.0\n\n\n9\n10\nmale\nTrue\n20.0\n\n\n10\n1\nfemale\nFalse\n0.9\n\n\n11\n2\nfemale\nFalse\n1.8\n\n\n12\n3\nfemale\nFalse\n2.7\n\n\n13\n4\nfemale\nFalse\n3.6\n\n\n14\n5\nfemale\nFalse\n4.5\n\n\n15\n6\nfemale\nFalse\n5.4\n\n\n16\n7\nfemale\nFalse\n6.3\n\n\n17\n8\nfemale\nFalse\n7.2\n\n\n18\n9\nfemale\nTrue\n16.2\n\n\n19\n10\nfemale\nTrue\n18.0"
  },
  {
    "objectID": "posts/2019-11-29_gender_gap.html#sec-honest-gg",
    "href": "posts/2019-11-29_gender_gap.html#sec-honest-gg",
    "title": "How NOT to calculate gender gaps in wages",
    "section": "Honest Gender Gaps",
    "text": "Honest Gender Gaps\n\n\nCode\nhonest_gg = data.groupby('gender').mean()\nhgg = (\n  honest_gg.loc[\"female\", \"wages\"] - honest_gg.loc[\"male\", \"wages\"]\n  )/honest_gg.loc[\"male\", \"wages\"]\n\n\nWe all agree that there is a sizable amount of gender discrimination in this example. This staggering amount of discrimination should be detectable in my simulated wages. And rightly enough, the raw gender gap between genders is -29.89% – that is, women earn about 30% less than men. This makes sense, as this gap represents the 10% pay cut I gave to each woman, plus an additional component driven by the discrimination in promotions."
  },
  {
    "objectID": "posts/2019-11-29_gender_gap.html#adjusted-gender-gaps",
    "href": "posts/2019-11-29_gender_gap.html#adjusted-gender-gaps",
    "title": "How NOT to calculate gender gaps in wages",
    "section": "“Adjusted” Gender Gaps",
    "text": "“Adjusted” Gender Gaps\n\nWait wait wait wait wait-a-minute!\nSays the clever pundit dude.\nCertainly you are making a naïve mistake! Certainly you can’t compare pears and apples! These people are doing different jobs. Certainly you should compare the gender gap within an occupation to obtain the true gender gap!\n\nWell… Let’s see what happens if we compare gender gaps within occupation titles.\n\n\nCode\nadj_gg = data.groupby(['gender', 'manager'])[\"wages\"].mean().reset_index().pivot(columns='gender', index='manager')\nadj_gg[\"Gender gap\"] = (\n  adj_gg.iloc[:, 0]-adj_gg.iloc[:, 1]\n)/adj_gg.iloc[:, 1]\n\nmanager_gg = adj_gg.loc[True, \"Gender gap\"].values[0]\nnmanager_gg = adj_gg.loc[False, \"Gender gap\"].values[0]\nadj_gg\n\n\n\n\n\n\n\n\n\nwages\nGender gap\n\n\ngender\nfemale\nmale\n\n\n\nmanager\n\n\n\n\n\n\n\nFalse\n4.05\n3.0\n0.35000\n\n\nTrue\n17.10\n16.0\n0.06875\n\n\n\n\n\n\n\nWhoa! Here it shows that are actually men being discriminated against! Women make more than men on average in each role. Like, among non-managers, females make a staggering 35% more than men! And even among managers women outearn the men by almost 7%!\n\nSee?\nSays the clever pundit dude\nIt’s men who need protection! We are cuddling women too much and now they exploit it and outearn men and men feel sad ☹️"
  },
  {
    "objectID": "posts/2019-11-29_gender_gap.html#adjusting-for-endogenous-variable-will-bias-your-results",
    "href": "posts/2019-11-29_gender_gap.html#adjusting-for-endogenous-variable-will-bias-your-results",
    "title": "How NOT to calculate gender gaps in wages",
    "section": "Adjusting for endogenous variable will bias your results",
    "text": "Adjusting for endogenous variable will bias your results\nSo, how is are these results possible when I was discriminating like a maniac just a few lines above? Well, that’s because we are controlling for an endogenous variable, that is, job title. Because whether you are a manager or not depends not only on your ability but also on your gender, this control introduces a subtle type of bias.\n\n\n\n\n\n\n\n\n\nflowchart\n  Gender[Gender] ==&gt; Title(Job title)\n  Gender ==&gt; Wage\n  Ability[Ability] -.-&gt; Title\n  Title ==&gt; Wage(Wage)\n  Ability -.-&gt; Wage\n  linkStyle 0,1,3 stroke:green\n  style Gender fill:#a7f5b4\n  style Wage fill:#ff8f87\n\n\n Regressing wage on gender identifies the gross effect, i.e. the green flow… \n\n\n\n\n\n\n\n\nflowchart\n  Gender[Gender] ==&gt; Title(Job title)\n  Gender ==&gt; Wage\n  Ability[Ability] ==&gt; Title\n  Title --&gt; Wage(Wage)\n  Ability ==&gt; Wage\n  linkStyle 1 stroke:green\n  linkStyle 0,2,4 stroke:red\n  style Gender fill:#a7f5b4\n  style Wage fill:#ff8f87\n  style Title fill:#8ba9fc\n\n\n … but adjusting for job title opens a back-door path through ability (red) \n\n\n\n\n\n\nFigure 2: A Directed Acyclic Graph (DAG) allows to visualize why the bias occurrs if we adjust for job title, but not ability\n\n\n\nFigure 2 visualizes the causal relationships in our simulated example. Gender and ability both affect job title and wages. Job title in turn affects wages as well. While ability is inherently unobservable (there are multiple dimensions of ability), job title, wage, and gender are typically observable.\nThe right panel shows that if we regress wage on gender, i.e. we compute the honest gender gap as in Section 3, what we are actually identify is the full flow of effects going from gender to wage, highlighted in green in the left panel of Figure 2. That’s great! In our example, that’s the direct wage penalty of 10% plus the indirect effect from lower promotion rates.\nWhat happens however once we adjust for job title (shown in blue in the right panel)? Well, then we are blocking the effect going from job title to wages, sure. However, we are also opening a back-door path through ability (shown in red). So the total effect we end up estimating is the direct 10% effect, plus a bias component resulting from the fact that the abilities of workers within each job title–and thereby wages–will depend on gender. Because we are discriminating when promoting.\nConfused yet? Let’s look at what happens to the abilities of workers for each job title.\n\n\nCode\nab_gap = data.groupby(['gender', 'manager'])[\"ability\"].mean().reset_index().pivot(columns='gender', index='manager')\nab_gap\n\n\n\n\n\n\n\n\n\nability\n\n\ngender\nfemale\nmale\n\n\nmanager\n\n\n\n\n\n\nFalse\n4.5\n3.0\n\n\nTrue\n9.5\n8.0\n\n\n\n\n\n\n\nDespite men and women being on average absolutely identical in terms of ability, due to our selection and dicrimination into job titles, we get the apparently paradoxical result that women on average outsmart men within each occupation type. And because ability does matter for wages after all, we get a bias in out gender gap estimates: It’s natural that, on average, within each occupation group, women should earn more.\n\n\n\n\n\n\nAdjusting for something can be worse than not adjusting at all\n\n\n\nLet me stress this point, as it can appear counterintuitive at first. You might think that, intuitively, the more you adjust for stuff, the closest you get to a pure effect.\nNope. If you do not know what you are doing, and do not have a clear mental causal model, blindly adjusting for covariates in a kitchen-sink approach risks doing more harm than good."
  },
  {
    "objectID": "posts/2019-11-29_gender_gap.html#summing-up",
    "href": "posts/2019-11-29_gender_gap.html#summing-up",
    "title": "How NOT to calculate gender gaps in wages",
    "section": "Summing up",
    "text": "Summing up\nWhat should then be the proper measure of gender gap? Well, consider that we have 10 individual of each gender, each of them a doppelganger in terms of ability of each other. So for each doppelganger couple \\(j\\in{1,\\ldots,10}\\) we could compare the wage of each woman $ w^{f}_{j}$ with that of her male doppelganger \\(w^{m}_{j}\\), take the difference in wages, and average these numbers:\n\\[\n\\frac{1}{10}\\sum_{j=1}^{10}(w^{m}_{j} - w^{f}_{j}) = \\frac{1}{10}\\sum_{j=1}^{10}w^{m}_{j} - \\frac{1}{10}\\sum_{j=1}^{10}w^{f}_{j}\n\\]\nwhich is equal to, guess what, the raw difference in average wage -29.89%.\nThe bottom line is that quantifying discrimination (by gender, race, whatnot) in the labor market is hard. Terribly so.\nInterpreting comparisons other than raw gender gaps takes training and expertise, and in most cases such comparisons just muddy the waters and make gender gaps less interpretable. The raw gender gap is instead a very simple yet powerful statistics. It’s easily interpretable, and while everyone agrees it has multiple causes, it’s still highly informative.\nSo either prepare to discuss your detailed set of assumptions (for example through a DAG) or stick to that."
  },
  {
    "objectID": "posts/2024-12_TeachSQLinGradSchool.html",
    "href": "posts/2024-12_TeachSQLinGradSchool.html",
    "title": "Teach SQL in (grad) school",
    "section": "",
    "text": "On December 12 I gave a workshop to a small crowd for the series Workshops for Ukraine, organized by Dariia Mykhailyshyna.\nBesides being happy to a tiny little contribution to the Ukranian cause, it was a perfect opportunity to\nWhich brings me to one of my greatest wishes towards higher education (in Denmark, at least)."
  },
  {
    "objectID": "posts/2024-12_TeachSQLinGradSchool.html#teach-sql-in-grad-school",
    "href": "posts/2024-12_TeachSQLinGradSchool.html#teach-sql-in-grad-school",
    "title": "Teach SQL in (grad) school",
    "section": "Teach SQL in (grad) school",
    "text": "Teach SQL in (grad) school\nPushing SQL into university classes is one of my pet peeves. I am honored to be among the “industry panel” (a forum where prospective employers can give input to the direction of a course of studies) for the bachelor in cognitive data science and the master in social data science at Copenhagen University. Pushing SQL in class teachings is one of the points I bring back to each and every meeting.\nSQL has consistently been around for about 40 years, and nothing points to it being less relevant going forward. On the contrary, business analytics is increasingly dependent on it. Further, while once you had to set up a SQL database, and ideally a server, for taking advantage of database engines, recent advances in the availability of self-contained database engines like DuckDB make it accessible to anyone who can run an R or a python script. I have used SQL daily at both Danmark Nationalbank and Realkredit Danmark.\nYet none of the people I have ever hired directly after university knew it.\nIt’s a skill that has to be taught at work. It takes time, it takes errors, it prolongs the time gap between “just focus on learning” and “you can actually start helping me for real” for newly educated hires. At the same time, I have to make them unlearn all their pandas workflow, and smart pandas hacks they have taught themselves to solve issues that are really best solved by a good old SQL query.\nSo why is this blatant mismatch between data skills taught at unis and required outside of academia occurring?\nMy thesis is simple."
  },
  {
    "objectID": "posts/2024-12_TeachSQLinGradSchool.html#most-academic-economists-never-really-pick-up-sql",
    "href": "posts/2024-12_TeachSQLinGradSchool.html#most-academic-economists-never-really-pick-up-sql",
    "title": "Teach SQL in (grad) school",
    "section": "Most academic economists never really pick up SQL",
    "text": "Most academic economists never really pick up SQL\nAnd thereby, they fail to realize its usefulness.\nAnd that’s true even for data people, very good at coding in R or python or Julia or whatever else. I know, because I was one of them.\nIt’s a recurring circle of missed opportunities. Econ professors do not know SQL, so they don’t teach it at school. The students going to the private sector will learn it, but those pursuing an academic career are never exposed to it, and never realize what it can. At best, they have heard about it and regard it as “some sort of old-school, weird way to extract data from remote databases”. So when it’s their turn to teach data classes, they do not teach it.\n\n\n\n\n\nblock-beta\n  columns 3\n  space A([\"Professors do \\nnot know SQL\"]) space\n  B([\"Professors do not\\nteach SQL to\\ngrad students\"]) space D([\"Grad students\\nbecome professors\"])\n  space C([\"Grad students\\ndo not know SQL\"]) space\n  A --&gt; B \n  B --&gt; C \n  C --&gt; D\n  D --&gt; A\n\n\n\n The recurring circle of misssed opportunities \n\n\n\nAt the same time, compared to fields like law, or medicine, the “real world” of the private sectors is an absorbing state. People like me, going from academia to the private sector, do not ever go back, even just for teaching a class.\nThis post, maybe the first in a series, is meant to showcase why SQL ought really to be a staple in the toolbox of anyone working with data - in or outside academia. The focus of this post will be showcasing how SQL allows you to use your computing resources efficiently. And thereby, working with larger-than-RAM data.\n\n\n\n\n\n\nBut what about speed?\n\n\n\nA great seeling point of tools like DuckDB is that they allow you to do the same stuff you’d do in pandas or dplyr way faster.\nThere’s a lot that has been written on this already however, and I’d refer to this post by Grant McDermott if you want to know more.\nFurthermore, focus on efficiency is also a focus on speed. Using resource optimally is one of the main reasons database engines are fast at data processing."
  },
  {
    "objectID": "posts/2024-12_TeachSQLinGradSchool.html#but-isnt-sql-just-a-weird-way-to-download-data-from-a-remote-database",
    "href": "posts/2024-12_TeachSQLinGradSchool.html#but-isnt-sql-just-a-weird-way-to-download-data-from-a-remote-database",
    "title": "Teach SQL in (grad) school",
    "section": "But isn’t SQL just a weird way to download data from a remote database?",
    "text": "But isn’t SQL just a weird way to download data from a remote database?\n…And then do the real data work in python or R?\n\n\n\n\n\n\nNO\n\n\n\n\n\n\nThis is often the first misconception to eliminate. You can of course use SQL this way, like an ill-devised FTP tool. You can also choose to cut a steak with a scissor. Neither is a good idea: you’d be using the wrong tool for the intended purpose.\nThe point of SQL is very much not of downloading all available data from a remote server. In fact, SQL is used for the opposite: Selecting only the data you need and minimize the flow between where the data resides, and where it needs to be loaded to (your RAM).\nBut the second, understandable misconception, is that using SQL:\n\nRequires setting up a database of sorts\nIs kind of pointless when working with local data\n\n\n\nI'm still not sure I understand the benefits.\nIs it that the WHERE commands are so intuitive in SQL? I could do a similar query very easily in dplyr.\nIs it the size of the data? I could use feather in R or just work on the cluster.\n\n— Peter Deffebach (@macrodev.bsky.social) December 20, 2024 at 5:14 PM\n\n\nPeter’s question is a fair one. Why should we bother?"
  },
  {
    "objectID": "posts/2024-12_TeachSQLinGradSchool.html#use-your-computing-resources-efficiently",
    "href": "posts/2024-12_TeachSQLinGradSchool.html#use-your-computing-resources-efficiently",
    "title": "Teach SQL in (grad) school",
    "section": "Use your computing resources efficiently",
    "text": "Use your computing resources efficiently\nIn my opinion, the main point of SQL is that it is designed to use resources efficiently. Both when querying, and when storing data (hence relational databases).\nThe need for efficiency is often lost because of today’s superpowered hardware. During my PhD, I worked on a remote windows server with 5GB RAM available for data and shared across a dozen of so researchers. Today, a MacBook Pro ships with minimum 16GB RAM. Computers are extremely fast, and cloud computing is easily accessible.\nAnd yet, I have always disliked wasting resources. Why waiting minutes when a data tasks can be done in seconds? What if you encounter a truly large dataset? Are you willing to wait hours just because you are including in your workflow tons of data you do not actually need?\nIn the sections below I will use NYC’s yellow taxi cab data for a few examples. In line with Peter’s question above, I simply downloaded the parquet with October 2024 data and saved it locally. Imagine this as a very simple local data source.\n\n\nCode\nfrom pathlib import Path\nimport urllib.request\n\nsavedfile = Path.cwd() / 'yellow_tripdata_2024-10.parquet'\nif savedfile.is_file():\n  pass\nelse:\n  urllib.request.urlretrieve(\n      \"https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2024-10.parquet\",\n      \"yellow_tripdata_2024-10.parquet\"\n    )\n\n\nAs hinted several times already, I’ll be using duckdb to work with the data.\n\n\nCode\nimport duckdb\n\n\n\n\n\n\n\n\nWarning\n\n\n\nI use basic SELECT statements throughout the examples below. I do not explain these statements in this post, but I include the code for each example.\nI might write a blog post with an introduction to SELECT statements, but for now I refer to this notebook.\n\n\n\nLoad only the data you need\nThe whole point of using SQL is that you never need to load all of the available raw data in your system. That’s the whole point of data pipelines and data cleaning.\nThere’ll be columns you do not need, rows you need to exclude. You might think “but it takes no time anyway”. Sure, with our overpowered machines. But every time we do unnecessary operations we throw sand in the cogs of our machine.\nDoing so can be rational: After all, I do not code in C++. My time is also money. But loading tons of unnecessary data into our RAM, for discarding them right afterwards is a clear and easily avoidable waste of resources.\n\n\n\n\n\n\nBut I need to explore the data!\n\n\n\nIndeed.\nData exploration DOES NOT require loading the whole thing into the RAM. I definitely do not need to look at every single row and every single column to understand how I can use a dataset.\nMostly, you want to extract specific examples, doing some aggregates. All things SQL excels at.\n\n\nSo let’s see what the first five rows of the data we downloaded contains.\n\n\nCode\nduckdb.sql(\n  \"\"\"\n  select *\n  from 'yellow_tripdata_2024-10.parquet'\n  limit 5\n  \"\"\"\n)\n\n\n┌──────────┬──────────────────────┬───────────────────────┬─────────────────┬───────────────┬────────────┬────────────────────┬──────────────┬──────────────┬──────────────┬─────────────┬────────┬─────────┬────────────┬──────────────┬───────────────────────┬──────────────┬──────────────────────┬─────────────┐\n│ VendorID │ tpep_pickup_datetime │ tpep_dropoff_datetime │ passenger_count │ trip_distance │ RatecodeID │ store_and_fwd_flag │ PULocationID │ DOLocationID │ payment_type │ fare_amount │ extra  │ mta_tax │ tip_amount │ tolls_amount │ improvement_surcharge │ total_amount │ congestion_surcharge │ Airport_fee │\n│  int32   │      timestamp       │       timestamp       │      int64      │    double     │   int64    │      varchar       │    int32     │    int32     │    int64     │   double    │ double │ double  │   double   │    double    │        double         │    double    │        double        │   double    │\n├──────────┼──────────────────────┼───────────────────────┼─────────────────┼───────────────┼────────────┼────────────────────┼──────────────┼──────────────┼──────────────┼─────────────┼────────┼─────────┼────────────┼──────────────┼───────────────────────┼──────────────┼──────────────────────┼─────────────┤\n│        2 │ 2024-10-01 00:30:44  │ 2024-10-01 00:48:26   │               1 │           3.0 │          1 │ N                  │          162 │          246 │            1 │        18.4 │    1.0 │     0.5 │        1.5 │          0.0 │                   1.0 │         24.9 │                  2.5 │         0.0 │\n│        1 │ 2024-10-01 00:12:20  │ 2024-10-01 00:25:25   │               1 │           2.2 │          1 │ N                  │           48 │          236 │            1 │        14.2 │    3.5 │     0.5 │        3.8 │          0.0 │                   1.0 │         23.0 │                  2.5 │         0.0 │\n│        1 │ 2024-10-01 00:04:46  │ 2024-10-01 00:13:52   │               1 │           2.7 │          1 │ N                  │          142 │           24 │            1 │        13.5 │    3.5 │     0.5 │        3.7 │          0.0 │                   1.0 │         22.2 │                  2.5 │         0.0 │\n│        1 │ 2024-10-01 00:12:10  │ 2024-10-01 00:23:01   │               1 │           3.1 │          1 │ N                  │          233 │           75 │            1 │        14.2 │    3.5 │     0.5 │        2.0 │          0.0 │                   1.0 │         21.2 │                  2.5 │         0.0 │\n│        1 │ 2024-10-01 00:30:22  │ 2024-10-01 00:30:39   │               1 │           0.0 │          1 │ N                  │          262 │          262 │            3 │         3.0 │    3.5 │     0.5 │        0.0 │          0.0 │                   1.0 │          8.0 │                  2.5 │         0.0 │\n└──────────┴──────────────────────┴───────────────────────┴─────────────────┴───────────────┴────────────┴────────────────────┴──────────────┴──────────────┴──────────────┴─────────────┴────────┴─────────┴────────────┴──────────────┴───────────────────────┴──────────────┴──────────────────────┴─────────────┘\n\n\nCool. Just gotten the first 5 rows out. One could already see some patterns and some data of use.\nBut I wonder, can I describe all variables similarly to a pandas.DataFrame.describe()? With DuckDB, sure\n\n\nCode\nduckdb.sql(\n  \"\"\"\n  SUMMARIZE SELECT * from 'yellow_tripdata_2024-10.parquet'\n  \"\"\"\n).df()\n\n\n\n\n\n\n\n\n\ncolumn_name\ncolumn_type\nmin\nmax\napprox_unique\navg\nstd\nq25\nq50\nq75\ncount\nnull_percentage\n\n\n\n\n0\nVendorID\nINTEGER\n1\n6\n3\n1.767158236629157\n0.4231049313968447\n2\n2\n2\n3833771\n0.00\n\n\n1\ntpep_pickup_datetime\nTIMESTAMP\n2009-01-01 00:35:59\n2024-11-14 18:30:00\n1646536\nNone\nNone\n2024-10-09 14:26:10.066878\n2024-10-17 02:39:44.162982\n2024-10-24 17:46:05.259919\n3833771\n0.00\n\n\n2\ntpep_dropoff_datetime\nTIMESTAMP\n2009-01-01 01:29:28\n2024-11-14 18:41:41\n1673214\nNone\nNone\n2024-10-09 15:00:32.457971\n2024-10-17 04:35:02.69613\n2024-10-24 18:05:48.686127\n3833771\n0.00\n\n\n3\npassenger_count\nBIGINT\n0\n9\n11\n1.308449781329327\n0.76773460811274\n1\n1\n1\n3833771\n10.27\n\n\n4\ntrip_distance\nDOUBLE\n0.0\n366343.04\n3580\n5.123568288246284\n486.9214199492435\n1.0236470768272852\n1.7703947295351523\n3.408899640526454\n3833771\n0.00\n\n\n5\nRatecodeID\nBIGINT\n1\n99\n7\n2.3393857801850997\n10.972502420745363\n1\n1\n1\n3833771\n10.27\n\n\n6\nstore_and_fwd_flag\nVARCHAR\nN\nY\n2\nNone\nNone\nNone\nNone\nNone\n3833771\n10.27\n\n\n7\nPULocationID\nINTEGER\n1\n265\n290\n164.48797776393008\n64.3353498901592\n132\n161\n233\n3833771\n0.00\n\n\n8\nDOLocationID\nINTEGER\n1\n265\n272\n163.9748793029109\n69.61100435412126\n114\n162\n234\n3833771\n0.00\n\n\n9\npayment_type\nBIGINT\n0\n4\n5\n1.0994349949436208\n0.6603012949509145\n1\n1\n1\n3833771\n0.00\n\n\n10\nfare_amount\nDOUBLE\n-920.0\n1680.2\n8804\n19.720713955008453\n19.431893985781933\n9.30049177536528\n14.224609681307248\n23.136480969465936\n3833771\n0.00\n\n\n11\nextra\nDOUBLE\n-7.5\n16.0\n68\n1.400851519300449\n1.8259134524837513\n0.0\n1.0\n2.5\n3833771\n0.00\n\n\n12\nmta_tax\nDOUBLE\n-0.5\n10.5\n11\n0.4781669536339025\n0.1347045242309791\n0.5\n0.5\n0.5\n3833771\n0.00\n\n\n13\ntip_amount\nDOUBLE\n-67.45\n500.0\n3783\n3.4248225154810834\n4.1867407431190955\n0.0\n2.6928540734898987\n4.41006614684763\n3833771\n0.00\n\n\n14\ntolls_amount\nDOUBLE\n-76.38\n150.0\n1216\n0.584599502682299\n2.2606473061080656\n0.0\n0.0\n0.0\n3833771\n0.00\n\n\n15\nimprovement_surcharge\nDOUBLE\n-1.0\n2.0\n5\n0.9575214064689841\n0.26930320533013286\n1.0\n1.0\n1.0\n3833771\n0.00\n\n\n16\ntotal_amount\nDOUBLE\n-901.0\n1690.89\n17716\n28.429628798927773\n24.41309204976698\n16.09112832601518\n21.539362915884368\n31.333575348855142\n3833771\n0.00\n\n\n17\ncongestion_surcharge\nDOUBLE\n-2.5\n2.5\n6\n2.230652282233429\n0.8855332647216068\n2.5\n2.5\n2.5\n3833771\n10.27\n\n\n18\nAirport_fee\nDOUBLE\n-1.75\n1.75\n4\n0.150372716923517\n0.5092291673630285\n0.0\n0.0\n0.0\n3833771\n10.27\n\n\n\n\n\n\n\nEven though I won’t be doing speed comparisons, I must point out that this description is pretty much instantaneous. A similar operation in pandas would likely be very fast, but so far I have avoided loading the full dataset in memory! And while that overhead might not make such a difference with 3.8M rows,\n\nI am using a minimal amount of RAM\nIt will make a difference for datasets 10, 100, or 1000 times larger\n\nI know I have 3.8M trips. I also know there are clear outliers. The trip_distance variable has a max of 366343.04 (miles?), and a P75 of 3.4, and a mean of over 5.\nLet’s say that I want to do an analysis of taxi speed over time of day. I might want to exclude: * Weekends * Trips over 20 miles * Trips with 0 distance * Duration over a minute\nOverall, why should I load into my memory rows I already now know I do not have to use? So While it’s true that a row selection is pretty straightforward to implement not only in SQL, but also in pandas, excel, whataver, the key is that by passing the filter before we load the data into the RAM, we’re saving a lot of space, energy, time.\nBut also, why limiting outselves to rows? If I need only times and distance, why not only load those columns? And maybe computing MpH on the fly?\n\n\nCode\ndf = duckdb.sql(\n  \"\"\"\n  SELECT\n  tpep_pickup_datetime\n  , date_sub('second', tpep_pickup_datetime, tpep_dropoff_datetime) as duration_seconds\n  , trip_distance\n  , 3600*trip_distance/(date_sub('second', tpep_pickup_datetime, tpep_dropoff_datetime)) as mph\n  from 'yellow_tripdata_2024-10.parquet'\n  WHERE \n    trip_distance &lt;= 20 and trip_distance&gt;0 \n    and dayofweek(tpep_pickup_datetime) between 1 and 5\n    and date_sub('second', tpep_pickup_datetime, tpep_dropoff_datetime)&gt;60\n  \"\"\"\n).df()\nprint(f\"\"\"Loaded {df.shape[0]} out of {\n    duckdb.sql(\"select count(*) from 'yellow_tripdata_2024-10.parquet'\").fetchone()[0]\n    } rows. First five rows:\"\"\"\n  )\ndf.head()\n\n\nLoaded 2739362 out of 3833771 rows. First five rows:\n\n\n\n\n\n\n\n\n\ntpep_pickup_datetime\nduration_seconds\ntrip_distance\nmph\n\n\n\n\n0\n2024-10-01 00:30:44\n1062\n3.00\n10.169492\n\n\n1\n2024-10-01 00:12:20\n785\n2.20\n10.089172\n\n\n2\n2024-10-01 00:04:46\n546\n2.70\n17.802198\n\n\n3\n2024-10-01 00:12:10\n651\n3.10\n17.142857\n\n\n4\n2024-10-01 00:31:20\n280\n0.97\n12.471429\n\n\n\n\n\n\n\nBut we can take it a step further.\nDo I need the microdata at all? At the end of the day, if I just want to plot a graph showing average taxi speed over time of day, why don’t I aggregate already -without loading every single row in an (expensive) pandas.DataFrame()? We can just import the aggregate data directly. No need to waste memory!\n\n\nCode\n# group by {frequency} minutes\nfrequency = 10\ndf = duckdb.sql(\n  f\"\"\"\n  with trips as (\n  SELECT\n  tpep_pickup_datetime\n  , date_sub('second', tpep_pickup_datetime, tpep_dropoff_datetime) as duration_seconds\n  , trip_distance\n  , 3600*trip_distance/(date_sub('second', tpep_pickup_datetime, tpep_dropoff_datetime)) as mph\n  from 'yellow_tripdata_2024-10.parquet'\n  WHERE\n    trip_distance &lt;= 20 and trip_distance&gt;0 \n    and dayofweek(tpep_pickup_datetime) between 1 and 5\n    and date_sub('second', tpep_pickup_datetime, tpep_dropoff_datetime)&gt;60\n  )\n  select\n    {frequency}*((60*hour(tpep_pickup_datetime) + minute(tpep_pickup_datetime))//{frequency})/60 as timeday_c\n    , AVG(mph) as avg_speed\n    , count(*) as cnt\n  from trips \n  where tpep_pickup_datetime is not null\n  group by all\n  \"\"\"\n).df()\nprint(f\"\"\"Loaded {df.shape[0]} rows. First five rows:\"\"\")\ndf.head()\n\n\nLoaded 144 rows. First five rows:\n\n\n\n\n\n\n\n\n\ntimeday_c\navg_speed\ncnt\n\n\n\n\n0\n14.000000\n8.709100\n24804\n\n\n1\n14.333333\n8.567473\n25610\n\n\n2\n16.833333\n8.447701\n28748\n\n\n3\n18.333333\n8.766163\n34094\n\n\n4\n20.500000\n11.267965\n27714\n\n\n\n\n\n\n\nAnd plot the resulting rows. Ta-dah. Don’t need to see the microdata really for this kind of descriptive stats.\n\n\nCode\nimport matplotlib.pyplot as plt\n\nfig, ax = plt.subplots(1, 1, figsize=(8, 5))\nax.scatter(df[\"timeday_c\"], df[\"avg_speed\"], s=50*df[\"cnt\"]/df[\"cnt\"].max())\nax.spines[['right', 'top']].set_visible(False)\nax.set_xticks(range(0, 25, 1))\nax.set_yticks(range(0, 25, 2 ))\nax.grid(axis='x')\nax.set_ylim(0, 22)\nax.set_xlim(0, 24)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Average taxi speed in NYC is has been about 9 MpH during working hours in October ’24\n\n\n\n\n\n\n\n\n\n\n\nIf you read this far…\n\n\n\n…you would probably like to plot the same graph for weekends. Try to import it as well as a groupby aggregate 😉. It’s an extra 3 lines in the same query!\n\n\n\n\nJoin, join, join\nNow, the real deal: SQL allows you to do operations that would be very hard - or downright impossible, as they’d overflow your RAM - without. Very often, these operations are about joining different tables together and filtering.\nSQL was after all meant to query relational databases. The idea behind relational databases is efficiency. In my experience, their structure appears strange at the eyes of economists. We have been trained to think of orderly panel datasets, NxT, large matrixes ready for regressions. In reality, most data look like the example below.\n\n\n\nExample of a relational database\n\n\nThe data is not stored “ready for analytics”. If you want to, say, regress moneySpent on price, maie controlling for employeeID, you need to join and patch 4 tables together. “SQL” (OLAP engines) is precisely constructed to make those joins on the fly as intuitive, quick, and painless as possible.\nOf course, the example alone is a simplistic example. Tipically, data warehouses have hundreds of connected tables, all storing different pieces of information. ANd that’s relevant for you: Because this is the environment your students will meet if they get a job as an analyst in the private sector.\nJOINs in SQL are therefore necessarily more powerful than .merge() operations in pandas. Think of JOINs as constructing behind the curtains all possible combinations across the tables you need to join, and applying filters to select the correct rows. So the ON conditions in JOINs are pretty much equivalent to WHERE statements.\nThe consequence is that with SQL you can easily do unequal joins (t1.var1&gt;t2.var2), joins on functions of variables, and so on. These types of JOINs are way more common than one might think.\nTake as an example the table linking clients to primary bank adviser I worked with last week. Its structure is something like the following\n\nAn example of a client-adviser relationship table\n\n\nClientID\nAdviserID\nValidFrom\nValidTo\n\n\n\n\nA\n1\n2021-01-14\n2021-08-03\n\n\nA\n2\n2021-08-03\n2024-06-01\n\n\nA\n3\n2024-06-01\n9999-12-31\n\n\nB\n2\n2023-12-01\n9999-12-31\n\n\n\nTo find out which adviser the client had when they redeemed the mortgage, I would need to do a JOIN based on timestamps.\nselect \n  r.clientID\n  , r.redemption_date\n  , r.other_stuff\n  , a.AdviserId\nfrom redemptions as r\nleft join advisers as a\n  on a.ClientId=r.ClientID\n  and r.redemption_time &gt;= r.ValidFrom\n  and r.redemption_time &lt; r.ValidTo\nIf I had the same tables to join in pandas I would have to proceed in two steps: 1) Join the tables on ClientID 2) Subset the result on the temporal conditions\nHowever, the first join might make the number of rows to explode - going past that step might not even be possible given memory limits.\nBut does that matter for the work of an academic economist?\nIt might. For example, think of matching on observables (or identifying a control group.) In our taxi data, let’s say I want to measure the effect of number of passengers on tip rate. And my identification strategy tells me that as long as pick-up, delivery locations, day of the week, time of the day, and airport fee are the same, and distance is within a mile, the number of passenger is randomly assigned. To ensure treatments and controls are not replicated, we can even pick potential treatments as those starting a trip on odd minutes, potential controls as those starting a trip on even minutes.\n\nBear with me, it’s the first example I could think of. I assume you’ll have a better identification strategy.\n\nHow would you find a matched control? Well, a single SQL query can do the job.\n\n\nCode\n%%time\ndf = duckdb.sql(\n  \"\"\"\n  with valid_rides as (\n  SELECT *\n  from 'yellow_tripdata_2024-10.parquet'\n  WHERE \n    trip_distance &lt;= 20 and trip_distance&gt;0 \n    and dayofweek(tpep_pickup_datetime) between 1 and 5\n    and date_sub('second', tpep_pickup_datetime, tpep_dropoff_datetime)&gt;60\n    and passenger_count&gt;0\n  )\n  select\n    T.tpep_pickup_datetime as pickup_treat\n    , C.tpep_pickup_datetime as pickup_control\n    , T.tpep_dropoff_datetime as dropoff_treat\n    , C.tpep_dropoff_datetime as dropoff_control\n    , T.passenger_count as passenger_count_treat\n    , C.passenger_count as passenger_count_control\n    , T.tip_amount/T.total_amount as tip_share_treat\n    , C.tip_amount/C.total_amount as tip_share_control\n  from valid_rides as T\n  inner join valid_rides as C\n    ON\n    -- locations must be the same\n    T.PULocationID=C.PULocationID and T.DOLocationID=C.PULocationID\n    -- Airport fee the same\n    and T.Airport_fee=C.Airport_fee\n    -- Same day, same hour\n    and date_trunc('hour', T.tpep_pickup_datetime)=date_trunc('hour', C.tpep_pickup_datetime)\n    -- close enough distance\n    and T.trip_distance between C.trip_distance -0.5 and C.trip_distance +0.5\n    -- same payment type\n    and T.payment_type=C.payment_type\n    -- treated must start at odd minutes\n    and minute(T.tpep_pickup_datetime)%2=1\n    -- controls must start at even minutes\n    and minute(C.tpep_pickup_datetime)%2=0\n    -- and finally remove the same trip\n    and T.tpep_pickup_datetime != C.tpep_pickup_datetime\n    and T.tpep_dropoff_datetime != C.tpep_dropoff_datetime\n  --order by T.tpep_dropoff_datetime\n  \"\"\"\n).df()\nprint(f\"\"\"Loaded {df.shape[0]} rows. First five rows:\"\"\")\ndf.head()\n\n\nLoaded 1241469 rows. First five rows:\nCPU times: user 2.03 s, sys: 181 ms, total: 2.21 s\nWall time: 623 ms\n\n\n\n\n\n\n\n\n\npickup_treat\npickup_control\ndropoff_treat\ndropoff_control\npassenger_count_treat\npassenger_count_control\ntip_share_treat\ntip_share_control\n\n\n\n\n0\n2024-10-29 15:07:38\n2024-10-29 15:48:27\n2024-10-29 15:16:00\n2024-10-29 15:59:01\n1\n1\n0.167832\n0.000000\n\n\n1\n2024-10-29 15:35:37\n2024-10-29 15:02:41\n2024-10-29 15:45:24\n2024-10-29 15:07:57\n1\n1\n0.166667\n0.199695\n\n\n2\n2024-10-29 15:21:26\n2024-10-29 15:18:28\n2024-10-29 15:32:50\n2024-10-29 15:27:39\n1\n2\n0.000000\n0.000000\n\n\n3\n2024-10-29 15:23:56\n2024-10-29 15:06:54\n2024-10-29 15:29:49\n2024-10-29 15:11:40\n1\n1\n0.000000\n0.000000\n\n\n4\n2024-10-29 15:37:35\n2024-10-29 15:18:28\n2024-10-29 15:41:04\n2024-10-29 15:27:39\n1\n2\n0.000000\n0.000000\n\n\n\n\n\n\n\nGranted, it’s a relatively complex query - but does the job nicely. And here I couldn’t help it but printing out the time it took as well.\nSo, drum rolls… is there any effect?\nOf course not, it was a stupid idea.\n\n\nCode\nduckdb.sql(\n  \"\"\"\n  select least(passenger_count_treat, 5) as passenger_count_treat\n  , least(passenger_count_control, 5) as passenger_count_control\n  , avg(tip_share_treat - tip_share_control) as effect\n  from df \n  group by all\n  \"\"\"\n).df().pivot(columns='passenger_count_control', index='passenger_count_treat')\n\n\n\n\n\n\n\n\n\neffect\n\n\npassenger_count_control\n1\n2\n3\n4\n5\n\n\npassenger_count_treat\n\n\n\n\n\n\n\n\n\n1\n0.001226\n0.001432\n0.003868\n0.003036\n0.000215\n\n\n2\n-0.000163\n-0.000303\n0.001733\n0.004597\n0.002828\n\n\n3\n-0.000645\n0.001073\n0.004201\n0.004506\n-0.002845\n\n\n4\n-0.003199\n-0.005196\n-0.007590\n-0.007027\n-0.013053\n\n\n5\n0.002610\n-0.000297\n0.004864\n-0.001569\n-0.000914\n\n\n\n\n\n\n\nBut, as always…\n\n\n\nAlways."
  },
  {
    "objectID": "posts/2024-12_TeachSQLinGradSchool.html#summing-up",
    "href": "posts/2024-12_TeachSQLinGradSchool.html#summing-up",
    "title": "Teach SQL in (grad) school",
    "section": "Summing up",
    "text": "Summing up\n\nSQL is useful.\n\nIt allows you to spare your machine for unnecessary data loads into expensive formats, such as pandas.DataFrames.\nIt allows you do to cool joins that you did not dream of doing\n\nThanks to DuckDB, you can easily run SQL queries on your local data.\n\nIt has both a python and R interface\n\nYour students will need SQL in the private job market 🙏\n\nIf you work with students, do yourself, your students, and the whole industry a favor. Learn SQL and introduce it in college/university classes.\nThanks!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alessandro Tang-Andersen Martinello",
    "section": "",
    "text": "Originally from Jesolo, I obtained my PhD in Economics at the University of Copenhagen, and have worked both in academia at Lund University and in policy at the Danish Central Bank.\nI am now Head of Data and Analytics at Realkredit Danmark, part of Danske Bank group.\nHere you can find both my (very infrequenctly updated) blog and my old research projects."
  },
  {
    "objectID": "tidbits.html",
    "href": "tidbits.html",
    "title": "Tidbits",
    "section": "",
    "text": "Teach SQL in (grad) school\n\n\n\n\n\nWhy SQL ought really to be a staple in the toolbox of anyone working with data - in or outside academia.\n\n\n\n\n\nDec 21, 2024\n\n\n16 min\n\n\n\n\n\n\n\n\n\n\n\n\nFive focus points for AI governance\n\n\n\n\n\n\n\n\n\n\n\nApr 25, 2022\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nUsing DST’s API from python\n\n\n\n\n\nLearn how to extract data from Danmark Statistiks Statistikbanken with python\n\n\n\n\n\nFeb 4, 2022\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nHow NOT to calculate gender gaps in wages\n\n\n\n\n\nDo not adjust for job type and title. Just don’t.\n\n\n\n\n\nNov 29, 2019\n\n\n8 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-04-25_AI_5_focus_points.html",
    "href": "posts/2022-04-25_AI_5_focus_points.html",
    "title": "Five focus points for AI governance",
    "section": "",
    "text": "You might know that the European Commission has submitted a proposal for an AI act.\nThe proposal defines high-risk AI systems, requires minimum governance structures for such systems, and imposes large fines for lack of compliance, up to 6% of a firm’s global turnover.\nThe proposal as it stands will have consequences for the financial sector. Among high-risk systems, the act specifically includes systems for credit risk scores applied to natural persons - not only those using machine learning, but also those using standard statistical tools (logit, OLS, whatnot).\nAt Danmarks Nationalbank we have studied both the academic and non-academic literature, and consulted with national and international experts to understand the challenges and best practices in using AI in the financial sector. While machine learning is not a new paradigm for banks and financial institutions, who are used to exploit microdata and models to inform their business, it can amplify existing risks.\nIn this memo we summarize our findings, and suggest five focus points for financial institution to consider when moving from static statistical models to self-learning, dynamic AI systems.\n\n\n\nFive focus points for AI governance\n\n\nHave a read, and if you have any input, let us know!"
  },
  {
    "objectID": "posts/Using DSTs API with python.html",
    "href": "posts/Using DSTs API with python.html",
    "title": "Using DST’s API from python",
    "section": "",
    "text": "I am a huge fan of Denmark Statistics. Their Statistikbanken contains a wealth of data on the Danish society, economy, and population.\nNot only all these data are publicy available, but DST has for years also provided access to all their published data tables through an API, documented here. The API access makes it extremely easy to access and use data. Yet unless one has already some experience in accessing APIs, using it might be complex for an occasional student or analyst.\nThis notebook provides a quick guide on how to access data from DST’s Statsbanken through their API, and presents a utility class I wrote to more easily access data tables for analytical purposes.\nThe only explicit dependency of that utility is pandas, which is anyway an extremely widespread package.\nBoth notebook and class can be found at this GitHub repository.\nThe utility can be installed by\n# Start by importing necessary packages\nimport requests\nimport pandas as pd\nfrom IPython.display import display\nfrom io import StringIO\n\nfrom dstapi import DstApi # The helper class\nDST’s API has four separate function calls to programmatically navigate around the published tables. This guide assumes that the analysist has scouted Statistikbanken already, and has identified the one or two tables from which data should be extracted. For these purposes, we only need two function calls: tableinfo and data.\nThe standard process is to begin by obtaining the necessary information from tableinfo, and then construct the call to pass to data.\nThis guide will proceed by for each step of the process first showing how to do it by directly using requests (and pandas), and second showing how the utility class DstApi can facilitate the process."
  },
  {
    "objectID": "posts/Using DSTs API with python.html#step-1-understand-what-a-table-has-to-offer-and-how-it-is-structured",
    "href": "posts/Using DSTs API with python.html#step-1-understand-what-a-table-has-to-offer-and-how-it-is-structured",
    "title": "Using DST’s API from python",
    "section": "Step 1: Understand what a table has to offer and how it is structured",
    "text": "Step 1: Understand what a table has to offer and how it is structured\nOur primary example will be DST’s table METROX1, which reports an index measuring the weekly amount of passengers travelling by metro in Copenhagen. This index was developed to measure the population’s response to the COVID pandemic. The table is small and simple, allowing for quick experimentation.\n\nThe hard way\nAs we know the table’s name/id we can start by accessing the API directly through the python package requests, and ask about the table’s metadata (tableinfo).\nAn API call is composed by a main web address, a function call, and a set of parameters. The main web address is https://api.statbank.dk/v1. The function call in this case is tableinfo. The set of necessary parameters, per documentation, is the id of the table and the format in which we’d like to receive the information. We’ll pick \"metrox1\" for the first (note that the table-id parameter is case-sensitive), and \"JSON\" for the second.\nThe API at DST can be called through both requests.get() and requests.post(). DST’s documentation recommends using post, because as the number and complexity of parameters grows (with some of them containing non-standard Danish characters) it’s harder to embed them in an URL. However, as the call to tableinfo is simple, below I provide examples of using both methods.\nNote that the .json() method of the request.Response object serves to return the response content (which we requested in JSON format) rather than the object itself. That’s just to print out the output in the notebook.\nThis function returns a wealth of information. Not just the table id and description, but also the contact of the statistics responsible, and, crucially, names and values of the variables defining the table. In this case SÆSON and Tid.\nThe code below shows how to get the table’s metadata, and prints the beginning of the JSON file returned.\n\n\nShow the code\n# Directly embed parameters in the URL with response.get()\nrequests.get('https://api.statbank.dk/v1' + '/tableinfo' + \"?id=metrox1&format=JSON\").json()\n\n# Pass a dictionary of parameters to requests.get()\nparams = {'id': 'metrox1', 'format': 'JSON'}\nrequests.get('https://api.statbank.dk/v1' + '/tableinfo', params=params).json()\n\n# Use response.post() - note the change in the name of the parameter about the table's name\n# I'm also adding here a language parameter - most tables are available in both Danish and English\nparams = {'table': 'metrox1', 'format': 'JSON', 'lang':'en'}\ntable_metadata = requests.post(\n    'https://api.statbank.dk/v1' + '/tableinfo', json=params\n).json()\nprint(str(table_metadata).replace(',', ',\\n')[0:500] + '\\n...')\n\n\n{'id': 'METROX1',\n 'text': 'Workday passenger index in the Copenhagen Metro (experimental statistics)',\n 'description': 'Workday passenger index in the Copenhagen Metro (experimental statistics) by seasonal adjustment and time',\n 'unit': 'Index',\n 'suppressedDataValue': '0',\n 'updated': '2022-06-16T08:00:00',\n 'active': False,\n 'contacts': [{'name': 'Peter Ottosen',\n 'phone': '+4530429191',\n 'mail': 'pot@dst.dk'}],\n 'documentation': None,\n 'footnote': {'text': 'Data are indexed against an averag\n...\n\n\nThis wealth of information is already fantastic. In that metadata there’s pretty much anything you need to figure out if you can actually use the table, and eventually how you want to select the data (seasonally ajusted? For 2020 only?). Yet that JSON file might be tough to digest, especially for more complex tables. Those cases might require preprocessing and a different type of visualization. That’s where the DstApi helper class comes into play.\n\n\nThe easy way\nDstApi has two methods for examining metadata.\nThe first one, tablesummary, summarizes the main metadata information: * The id and description of the table * The last update time * A table with the main available cuts of the data. Each row corresponds to a variable against which we can select, with examples of variable values and labels\n\n\nShow the code\n# Initialize the class with the target table\nmetro = DstApi('METROX1')\n\n# Get the table summary\nmetro.tablesummary(language='en')\n\n\nTable METROX1: Workday passenger index in the Copenhagen Metro (experimental statistics) by seasonal adjustment and time\nLast update: 2022-06-16T08:00:00\n\n\n\n\n\n\n\n\n\nvariable name\n# values\nFirst value\nFirst value label\nLast value\nLast value label\nTime variable\n\n\n\n\n0\nSÆSON\n2\n10\nSeasonally adjusted\n11\nNon-seasonally adjusted\nFalse\n\n\n1\nTid\n122\n2020U01\n2020U01\n2022U23\n2022U23\nTrue\n\n\n\n\n\n\n\nThe second method variable_levels zooms into a specific variable and returns a dataframe for each potential variable value. For example, we could check each value of SÆSON\n\n\nShow the code\nmetro.variable_levels('SÆSON', language='en')\n\n\n\n\n\n\n\n\n\nid\ntext\n\n\n\n\n0\n10\nSeasonally adjusted\n\n\n1\n11\nNon-seasonally adjusted\n\n\n\n\n\n\n\nNow, we already knew these values for this simple table from tablesummary(), as they are only two. But for more complex tables, this method is very handy. Take for example DNVPDKR2, a table showing the circulating amount of mortgage bonds issued by Danish mortgage institutes.\nIf I wanted for example to extract only data about fixed interest rate, convertible bonds it would be hard to know I should be referring to the value FK in advance.\nBut I can use\n\nmethod .tablesummary() to see which variables you can select on\nmethod .variable_levels() to see which values are available for each variable. Here * is a wildcard that selects all available values for the variable.\n\n\n\nShow the code\ndnrk = DstApi('DNVPDKR2')\ndnrk.tablesummary(language='en')\n\n\nTable DNVPDKR2: Danish mortgage bonds by type of mortgage bond, original maturity, remaining maturity, coupon (nominal interest rate), currency, issuer, investor sector, covered bonds, data type and time\nLast update: 2024-11-28T08:00:00\n\n\n\n\n\n\n\n\n\nvariable name\n# values\nFirst value\nFirst value label\nLast value\nLast value label\nTime variable\n\n\n\n\n0\nTYPREAL\n9\nA0\nAll mortgage bonds\nO\n1.6 Other mortgage bonds\nFalse\n\n\n1\nLØBETID3\n7\nA0\nAll original maturities\n6\nOther (unspecified)\nFalse\n\n\n2\nLØBETID2\n9\nA0\nAll remaining maturities\n8\nOther (unspecified)\nFalse\n\n\n3\nKUPON2\n15\nA0\nAll coupons\nN\nOther coupons\nFalse\n\n\n4\nVALUTA\n6\nA0\nAll currencies\nO\nOther\nFalse\n\n\n5\nUDSTED\n10\nA0\nAll issuers\nO\nOther issuers\nFalse\n\n\n6\nINVSEKTOR\n10\nA0\nAll sectors\nU\n2. Foreign (S.2)\nFalse\n\n\n7\nDAEKOBL\n4\nA0\nAll mortgage bonds\nRO\nRO (other mortgage bonds)\nFalse\n\n\n8\nDATAT\n5\nN1\nStock - Nominal\nB3\nValue adjustments - Market value\nFalse\n\n\n9\nTid\n299\n1999M12\n1999M12\n2024M10\n2024M10\nTrue\n\n\n\n\n\n\n\n\n\nShow the code\ndnrk.variable_levels('TYPREAL', language='en')\n\n\n\n\n\n\n\n\n\nid\ntext\n\n\n\n\n0\nA0\nAll mortgage bonds\n\n\n1\nFK\n1.1 Fixed rate convertible bonds\n\n\n2\nFKU\n- - 1.1.1 Open for issue - fixed rate converti...\n\n\n3\nFKE\n- - 1.1.2 No longer open for issue - fixed rat...\n\n\n4\nI\n1.2 Indexed bonds\n\n\n5\nRTL\n1.3 Adjustable rate bonds (RTL bonds)\n\n\n6\nV\n1.4 Bonds with a rererence rate (without inter...\n\n\n7\nVR\n1.5 Bonds with a reference rate (with interest...\n\n\n8\nO\n1.6 Other mortgage bonds"
  },
  {
    "objectID": "posts/Using DSTs API with python.html#step-2-get-the-data-you-need",
    "href": "posts/Using DSTs API with python.html#step-2-get-the-data-you-need",
    "title": "Using DST’s API from python",
    "section": "Step 2: Get the data you need",
    "text": "Step 2: Get the data you need\nThe first step is essential for designing this second step. First and foremost because we need that information to design the call to data. Second, to make sure we only get out the data we need. Asking for too much data only to then having to throw half of it out locally is wasteful, and ultimately disrespectful for the resources invested into allowing anyone to fire up an API call (I mean how amazing is that?).\n\nThe hard way\nAs for the first step, we’ll start by doing it manually. Here I’ll rely exclusively on request.post() as recommended by DST.\nTo select the query parameters to pass to the data function appropriatedly one ought to have a careful look at the DATA section in the documentation. Nonetheless, hopefully the examples below will serve to clarify how to construct such calls.\nThe first two key parameters are, as before, the table name and the format in which we’d like to obtain the data. In the examples below I choose BULK, which has the advantage of being faster and allowing an unlimited number of data rows at export. There are some limitations with this format, such as the inability to perform simple computations (e.g. sums) on the fly. If you need these utilities, you probably don’t need this guide, so I’ll stick with BULK here.\nThe third crucial parameter is the selection based on the variables shown in e.g. DstApi.tablesummary(). These are mandatory: we need to specify the selection we want to do. We might however choose to include a range of possible values, or all of them, in a selection. In this case, the character * acts as a joker. So to select all values of a variable, we can use *. To select all 2020 weeks in Tid, we could use 2020*.\nBelow I write the parameters necessary to download the seasonally adjusted (code 10) index for all weeks in the data, and pass them to requests.post(). Finally I print the first 200 characters of the data we received back (in ;-separated format).\n\nparams = {\n    'table': 'metrox1',\n    'format': 'BULK',\n    'variables': [\n        {'code': 'SÆSON', 'values': ['10']},\n        {'code': 'Tid', 'values': ['*']}\n    ]\n}\nr = requests.post('https://api.statbank.dk/v1' + '/data', json=params)\nprint(r.text[:200])\n\nSÆSON;TID;INDHOLD\nSæsonkorrigeret;2020U01;37,7\nSæsonkorrigeret;2020U08;105,0\nSæsonkorrigeret;2020U09;95,2\nSæsonkorrigeret;2020U10;93,0\nSæsonkorrigeret;2020U11;63,0\nSæsonkorrigeret;2020U12;17,9\n\n\n\nNeat! We can then save this data to a csv file or whatever, or directly import it into pandas:\n\n\nShow the code\npd.read_table(StringIO(r.text), sep=';').head()\n\n\n\n\n\n\n\n\n\nSÆSON\nTID\nINDHOLD\n\n\n\n\n0\nSæsonkorrigeret\n2020U01\n37,7\n\n\n1\nSæsonkorrigeret\n2020U08\n105,0\n\n\n2\nSæsonkorrigeret\n2020U09\n95,2\n\n\n3\nSæsonkorrigeret\n2020U10\n93,0\n\n\n4\nSæsonkorrigeret\n2020U11\n63,0\n\n\n\n\n\n\n\nKeep in mind that you can also specify intervals for time variables, as in the example below, where I also require the data to be exported in English.\n\n\nShow the code\nparams = {\n    'table': 'metrox1',\n    'format': 'BULK',\n    'lang': 'en',\n    'variables': [\n        {'code': 'SÆSON', 'values': ['11']},\n        {'code': 'Tid', 'values': ['&gt;2020U45&lt;=2020U52']}\n    ]\n}\ndf = pd.read_csv(\n    StringIO(\n        requests.post('https://api.statbank.dk/v1' + '/data', json=params).text\n    ), sep=';'\n)\ndf.head()\n\n\n\n\n\n\n\n\n\nSÆSON\nTID\nINDHOLD\n\n\n\n\n0\nNon-seasonally adjusted\n2020U46\n56.2\n\n\n1\nNon-seasonally adjusted\n2020U47\n55.5\n\n\n2\nNon-seasonally adjusted\n2020U48\n58.3\n\n\n3\nNon-seasonally adjusted\n2020U49\n57.6\n\n\n4\nNon-seasonally adjusted\n2020U50\n46.9\n\n\n\n\n\n\n\n\n\nThe easy-er way\nThe code above is already quite compact, but to avoid remembering how to import the data into pandas all the time, DstApi has a method to import the data directly into pandas given a parameter dictionary. So, for example, given the params dictionary defined above, we might call directly\n\n\nShow the code\nmetro.get_data(params=params)\n\n\n\n\n\n\n\n\n\nSÆSON\nTID\nINDHOLD\n\n\n\n\n0\nSæsonkorrigeret\n2020U01\n37.7\n\n\n1\nSæsonkorrigeret\n2020U08\n105.0\n\n\n2\nSæsonkorrigeret\n2020U09\n95.2\n\n\n3\nSæsonkorrigeret\n2020U10\n93.0\n\n\n4\nSæsonkorrigeret\n2020U11\n63.0\n\n\n...\n...\n...\n...\n\n\n117\nSæsonkorrigeret\n2022U19\n99.5\n\n\n118\nSæsonkorrigeret\n2022U20\n95.7\n\n\n119\nSæsonkorrigeret\n2022U21\n103.1\n\n\n120\nSæsonkorrigeret\n2022U22\n108.4\n\n\n121\nSæsonkorrigeret\n2022U23\n109.3\n\n\n\n\n122 rows × 3 columns\n\n\n\nthe .get_data() method has also the built-in option of downloading an entire data table by not passing any parameter dictionary. As mentioned above, this might be (very) wasteful. Some DST tables contain billions of data points. That’s why when used in this way the method asks for explicit confirmation before proceeding.\nHowever, creating the params dictionary itself can be challenging. As we have seen above with table DNVPDKR2, table structures can be complex, and creating the parameter dictionary manually can be cumbersome.\nThat’s why DstApi has a helper method returning a base dictionary of parameters with default values.\n\n\n\n\n\n\nOnly pass a default params to .get_data() if you know what you are doing\n\n\n\nSome tables in Statistikbanken have millions of records. Downloading them all through the api can take a lot of time, and it’s extremely wasteful if in fact you only need a fraction of the data.\n\n\n\n\nShow the code\n# Start by constructing a basic dictionary\ndnrk = DstApi('DNVPDKR2')\nparams = dnrk.define_base_params(language = 'en')\nparams\n\n\n{'table': 'dnvpdkr2',\n 'format': 'BULK',\n 'lang': 'en',\n 'variables': [{'code': 'TYPREAL', 'values': ['*']},\n  {'code': 'LØBETID3', 'values': ['*']},\n  {'code': 'LØBETID2', 'values': ['*']},\n  {'code': 'KUPON2', 'values': ['*']},\n  {'code': 'VALUTA', 'values': ['*']},\n  {'code': 'UDSTED', 'values': ['*']},\n  {'code': 'INVSEKTOR', 'values': ['*']},\n  {'code': 'DAEKOBL', 'values': ['*']},\n  {'code': 'DATAT', 'values': ['*']},\n  {'code': 'Tid', 'values': ['*']}]}\n\n\nOnce I have the basic structure, I can copy-paste the dictionary definition and use the method variable_levels to specify the data selection further. For example, I would like to have only bonds issued by Realkredit Danmark, so the code below tells me to use value RD for variable DAEKOBL.\n\n\nShow the code\ndnrk.variable_levels('LØBETID3', language='en')\n\n\n\n\n\n\n\n\n\nid\ntext\n\n\n\n\n0\nA0\nAlle oprindelige løbetider\n\n\n1\n1\nUnder 10 år\n\n\n2\n2\n10-årige\n\n\n3\n3\n15-årige\n\n\n4\n4\n20-årige\n\n\n5\n5\n30-årige\n\n\n6\n6\nAndet (uspecificeret)\n\n\n\n\n\n\n\nI can further refine my query filling in the selection parameters required and call the get_data() method to extract the final dataframe.\n\n\nShow the code\nparams = {'table': 'dnvpdkr2',\n 'format': 'BULK',\n 'lang': 'en',\n 'variables': [{'code': 'TYPREAL', 'values': ['FK']},\n  {'code': 'LØBETID3', 'values': ['5']},\n  {'code': 'LØBETID2', 'values': ['A0']},\n  {'code': 'KUPON2', 'values': ['A0']},\n  {'code': 'VALUTA', 'values': ['DKK']},\n  {'code': 'UDSTED', 'values': ['RD']},\n  {'code': 'INVSEKTOR', 'values': ['A0']},\n  {'code': 'DAEKOBL', 'values': ['A0']},\n  {'code': 'DATAT', 'values': ['N1']},\n  {'code': 'Tid', 'values': ['*']}]}\ndf = dnrk.get_data(params=params, language='en')\ndf.tail()[[\"TID\", \"INDHOLD\"]]\n\n\n\n\n\n\n\n\n\nTID\nINDHOLD\n\n\n\n\n294\n2024M06\n229104\n\n\n295\n2024M07\n228410\n\n\n296\n2024M08\n229411\n\n\n297\n2024M09\n231149\n\n\n298\n2024M10\n231694\n\n\n\n\n\n\n\nAnd just like that, I have the full time series of RD’s 30yo fixed interest rate bonds in nominal values.\nYou can play around with parameters in various ways. For example, here I select a range of weeks in 2020 from the metro table.\n\n\nShow the code\n# Start by constructing a basic dictionary\nparams = metro._define_base_params(language = 'en')\nparams['variables'][0]['values'] = ['10']\nparams['variables'][1]['values'] = ['&gt;2020U45&lt;=2020U52']\nmetro.get_data(params=params)\n\n\n\n\n\n\n\n\n\nSÆSON\nTID\nINDHOLD\n\n\n\n\n0\nSeasonally adjusted\n2020U46\n59.0\n\n\n1\nSeasonally adjusted\n2020U47\n54.0\n\n\n2\nSeasonally adjusted\n2020U48\n56.2\n\n\n3\nSeasonally adjusted\n2020U49\n54.5\n\n\n4\nSeasonally adjusted\n2020U50\n44.3\n\n\n5\nSeasonally adjusted\n2020U51\n40.2\n\n\n6\nSeasonally adjusted\n2020U52\n44.8"
  },
  {
    "objectID": "posts/Using DSTs API with python.html#and-thats-it",
    "href": "posts/Using DSTs API with python.html#and-thats-it",
    "title": "Using DST’s API from python",
    "section": "And that’s it!",
    "text": "And that’s it!\nI hope this guide was useful, and that the DstApi class can prove as helpful to you as it is for me.\nOnce again, let me conclude with a shout out to Denmark Statistics, a real national treasure. Thanks for all your work in gathering, organizing, and publishing data for everyone to use. It’s a fantastic service, and one for which you’ll never be thanked enough."
  }
]